import os
import json
import time
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm
import warnings
import psutil
from datetime import datetime

warnings.filterwarnings('ignore')


# ===================== 1. é…ç½®å‚æ•° =====================
class Config:
    # æ•°æ®é›†è·¯å¾„ (è¯·ç¡®ä¿è¿™é‡ŒæŒ‡å‘çš„æ˜¯åŒ…å«ä¿¡å·æ•°æ®çš„ç›®å½•)
    # å¦‚æžœä½ çš„åŽŸå§‹ .npy å·²ç»æ˜¯åŠ è¿‡å™ªå£°çš„ï¼Œå»ºè®®é‡æ–°ç”Ÿæˆä¸€ä»½"çº¯å‡€"æ•°æ®ï¼Œ
    # æˆ–è€…ç›´æŽ¥åœ¨çŽ°æœ‰æ•°æ®ä¸Šå åŠ æ›´å¤šå™ªå£°ï¼ˆè™½ç„¶ä¸ä¸¥è°¨ï¼Œä½†ä¹Ÿèƒ½èµ·åˆ°å¢žå¼ºä½œç”¨ï¼‰ã€‚
    DATASET_OUTPUT_DIR = "./modulation_dataset_50overlap"
    LOG_DIR = "./train_logs"

    # è®­ç»ƒè¶…å‚æ•°
    BATCH_SIZE = 64  # 48Gæ˜¾å­˜æŽ¨è 64~128
    EPOCHS = 50
    LR = 3e-4
    WEIGHT_DECAY = 1e-4
    ACCUMULATION_STEPS = 4

    # === æ–°å¢žï¼šSNR é…ç½® ===
    SNR_MIN = 10  # æœ€å°ä¿¡å™ªæ¯” dB
    SNR_MAX = 30  # æœ€å¤§ä¿¡å™ªæ¯” dB

    # è‡ªåŠ¨èŽ·å–çš„å‚æ•°
    NUM_CLASSES = 0
    SAMPLE_LENGTH = 4096

    # ç³»ç»Ÿå‚æ•°
    SAVE_INTERVAL = 5
    NUM_WORKERS = 0 if os.name == 'nt' else 4


config = Config()
os.makedirs(config.LOG_DIR, exist_ok=True)


# ===================== 2. å·¥å…·å‡½æ•° (åŠ å™ªæ ¸å¿ƒ) =====================
def log_info(msg, save_to_file=True):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_msg = f"[{timestamp}] {msg}"
    print(log_msg)
    if save_to_file:
        with open(os.path.join(config.LOG_DIR, "train_log.txt"), 'a', encoding='utf-8') as f:
            f.write(log_msg + "\n")


def monitor_resources():
    mem_info = "CPU"
    if torch.cuda.is_available():
        mem_alloc = torch.cuda.memory_allocated(0) / 1024 ** 3
        mem_res = torch.cuda.memory_reserved(0) / 1024 ** 3
        mem_info = f"GPU: {mem_alloc:.1f}/{mem_res:.1f}GB"
    ram_used = psutil.virtual_memory().percent
    return f"{mem_info} | RAM: {ram_used}%"


def add_awgn(signal, snr_db):
    """
    å¯¹ä¿¡å·æ·»åŠ é«˜æ–¯ç™½å™ªå£° (AWGN)
    input: signal (np.array) [2, L]
    input: snr_db (float) ä¿¡å™ªæ¯”
    output: noisy_signal (np.array) [2, L]
    """
    # 1. è®¡ç®—ä¿¡å·åŠŸçŽ‡ (P_signal)
    # ä¿¡å·é€šå¸¸æ˜¯å¤æ•°å½¢å¼ I+jQï¼Œè¿™é‡Œåˆ†å¼€ç®—çš„åŠŸçŽ‡å’Œæ˜¯ä¸€æ ·çš„
    # P = sum(x^2) / N
    signal_power = np.mean(np.sum(signal ** 2, axis=0))

    # 2. æ ¹æ® SNR è®¡ç®—å™ªå£°åŠŸçŽ‡ (P_noise)
    # SNR(dB) = 10 * log10(P_signal / P_noise)
    # => P_noise = P_signal / 10^(SNR/10)
    noise_power = signal_power / (10 ** (snr_db / 10.0))

    # 3. ç”Ÿæˆå™ªå£°
    # å™ªå£°éœ€è¦åˆ†é…åˆ° I å’Œ Q ä¸¤è·¯ï¼Œæ‰€ä»¥å•è·¯åŠŸçŽ‡è¦é™¤ä»¥ 2 (æˆ–è€…æ ‡å‡†å·®é™¤ä»¥ sqrt(2))
    noise_std = np.sqrt(noise_power / 2)
    noise = np.random.normal(0, noise_std, size=signal.shape)

    # 4. å åŠ 
    return signal + noise


# ===================== 3. æ•°æ®é›† (æ”¯æŒåœ¨çº¿éšæœºåŠ å™ª) =====================
class ModulationDataset(Dataset):
    def __init__(self, split='train', snr_range=(10, 30)):
        self.split = split
        self.snr_min, self.snr_max = snr_range

        self.data_path = os.path.join(config.DATASET_OUTPUT_DIR, f"{split}_data.npy")
        self.labels_path = os.path.join(config.DATASET_OUTPUT_DIR, f"{split}_labels.npy")

        if not os.path.exists(self.data_path):
            raise FileNotFoundError(f"âŒ {split}é›†æ–‡ä»¶ç¼ºå¤±ï¼š{self.data_path}")

        try:
            # mmap_mode='r': å†…å­˜æ˜ å°„
            self.data = np.load(self.data_path, mmap_mode='r')
            self.labels = np.load(self.labels_path, mmap_mode='r')
        except Exception as e:
            raise RuntimeError(f"âŒ åŠ è½½{split}é›†å¤±è´¥ï¼š{e}")

        self.num_samples = len(self.labels)

        # ä»…åœ¨è®­ç»ƒé›†å¼€å¯éšæœºåŠ å™ªï¼ŒéªŒè¯/æµ‹è¯•é›†é€šå¸¸ä½¿ç”¨å›ºå®šSNRæˆ–è€…ä¹Ÿéšæœº(è§†è¯„ä¼°éœ€æ±‚è€Œå®š)
        # è¿™é‡Œé»˜è®¤å…¨éƒ¨éšæœºï¼Œå¦‚æžœä½ å¸Œæœ›æµ‹è¯•é›†å›ºå®šï¼Œå¯ä»¥åœ¨ getitem é‡Œåˆ¤æ–­
        self.is_training = (split == 'train')

        log_info(f"âœ… Loaded {split}: {self.num_samples:,} samples | SNR Mode: {self.snr_min}~{self.snr_max} dB")

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        try:
            # 1. è¯»å–åŽŸå§‹æ•°æ® (å‡è®¾è¿™é‡Œæ˜¯çº¯å‡€æ•°æ®ï¼Œæˆ–è€…ä½Žå™ªæ•°æ®)
            # å¤åˆ¶ä¸€ä»½å‡ºæ¥ä»¥å…ä¿®æ”¹ mmap æºæ–‡ä»¶
            sample_np = self.data[idx].astype(np.float32).copy()
            label_val = self.labels[idx]

            # 2. åœ¨çº¿åŠ å™ª (On-the-fly Augmentation)
            # ç”Ÿæˆä¸€ä¸ª [Min, Max] ä¹‹é—´çš„éšæœº SNR
            current_snr = np.random.uniform(self.snr_min, self.snr_max)

            # è°ƒç”¨åŠ å™ªå‡½æ•°
            noisy_sample = add_awgn(sample_np, current_snr)

            return torch.from_numpy(noisy_sample.astype(np.float32)), torch.tensor(label_val, dtype=torch.long)

        except Exception as e:
            # å®¹é”™è¿”å›žé›¶å¼ é‡
            return torch.zeros(2, config.SAMPLE_LENGTH).float(), torch.tensor(0).long()


# ===================== 4. æ¨¡åž‹ç»„ä»¶ (ä¿æŒå¤šåŸŸ+æ³¨æ„åŠ›ä¸å˜) =====================

class Swish(nn.Module):
    def forward(self, x): return x * torch.sigmoid(x)


def get_activation():
    return nn.SiLU() if hasattr(nn, 'SiLU') else Swish()


class SEBlock1D(nn.Module):
    def __init__(self, channel, reduction=16):
        super(SEBlock1D, self).__init__()
        reduced_channel = max(channel // reduction, 4)
        self.avg_pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, reduced_channel, bias=False),
            get_activation(),
            nn.Linear(reduced_channel, channel, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1)
        return x * y.expand_as(x)


class MultiDomainEmbedding(nn.Module):
    def __init__(self):
        super().__init__()
        self.register_buffer('haar_weights', torch.tensor([
            [[0.70710678, 0.70710678]],
            [[0.70710678, -0.70710678]]
        ]).float())

    def forward(self, x):
        B, C, L = x.shape
        # FFT
        x_complex = torch.complex(x[:, 0, :], x[:, 1, :])
        fft_mag = torch.abs(torch.fft.fft(x_complex, dim=-1, norm='ortho'))
        fft_feature = torch.log1p(fft_mag).unsqueeze(1)

        # Wavelet
        x_reshaped = x.view(B * 2, 1, L)
        x_pad = F.pad(x_reshaped, (0, 1), mode='replicate')
        wavelet_out = F.conv1d(x_pad, self.haar_weights, stride=1)
        wavelet_feature = wavelet_out.view(B, 4, L)

        return torch.cat([x, fft_feature, wavelet_feature], dim=1)


class BLDE_1D_Modulation(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.embedding = MultiDomainEmbedding()
        self.features = nn.Sequential(
            nn.Conv1d(7, 32, 7, stride=2, padding=3, bias=False),
            nn.BatchNorm1d(32), get_activation(),
            SEBlock1D(32, reduction=4),

            nn.Conv1d(32, 64, 5, stride=2, padding=2, bias=False),
            nn.BatchNorm1d(64), get_activation(),
            SEBlock1D(64, reduction=8),
            nn.Dropout1d(0.1),

            nn.Conv1d(64, 128, 3, stride=2, padding=1, bias=False),
            nn.BatchNorm1d(128), get_activation(),
            SEBlock1D(128, reduction=16),

            nn.Conv1d(128, 256, 3, stride=2, padding=1, bias=False),
            nn.BatchNorm1d(256), get_activation(),
            SEBlock1D(256, reduction=16),

            nn.Conv1d(256, 512, 3, stride=2, padding=1, bias=False),
            nn.BatchNorm1d(512), get_activation(),
            SEBlock1D(512, reduction=16),

            nn.Conv1d(512, 512, 3, stride=2, padding=1, bias=False),
            nn.BatchNorm1d(512), get_activation(),
            SEBlock1D(512, reduction=16),
        )
        self.avgpool = nn.AdaptiveAvgPool1d(1)
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            get_activation(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.embedding(x)
        x = self.features(x)
        x = self.avgpool(x)
        return self.classifier(x)


# ===================== 5. è®­ç»ƒä¸»ç¨‹åº =====================
def train_model():
    log_info("=" * 60)
    log_info(f"ðŸš€ å¼€å§‹è®­ç»ƒ | Random SNR: {config.SNR_MIN}~{config.SNR_MAX} dB | Multi-Domain + SE")
    log_info("=" * 60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # è¯»å–é…ç½®
    json_path = os.path.join(config.DATASET_OUTPUT_DIR, "label_mapping.json")
    if os.path.exists(json_path):
        with open(json_path, 'r', encoding='utf-8') as f:
            mapping = json.load(f)
            if 'label_to_idx' in mapping:
                config.NUM_CLASSES = len(mapping['label_to_idx'])
            elif isinstance(mapping, dict):
                config.NUM_CLASSES = len(mapping)
    else:
        config.NUM_CLASSES = 20
    log_info(f"ðŸ“Œ Classes: {config.NUM_CLASSES}")

    # --- æ•°æ®åŠ è½½ (ä¼ é€’ SNR å‚æ•°) ---
    snr_range = (config.SNR_MIN, config.SNR_MAX)

    train_ds = ModulationDataset('train', snr_range=snr_range)
    # éªŒè¯é›†å’Œæµ‹è¯•é›†ä¹Ÿä¿æŒéšæœºSNRï¼Œä»¥æµ‹è¯•æ¨¡åž‹åœ¨åŠ¨æ€çŽ¯å¢ƒä¸‹çš„é²æ£’æ€§
    # å¦‚æžœæƒ³æµ‹è¯•çº¯å‡€ä¿¡å·ï¼Œå¯ä¿®æ”¹ä¸ºæžé«˜SNRï¼Œä¾‹å¦‚ (100, 100)
    val_ds = ModulationDataset('val', snr_range=snr_range)
    test_ds = ModulationDataset('test', snr_range=snr_range)

    train_loader = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True,
                              num_workers=config.NUM_WORKERS, pin_memory=True, drop_last=True)
    val_loader = DataLoader(val_ds, batch_size=config.BATCH_SIZE * 2, shuffle=False,
                            num_workers=config.NUM_WORKERS, pin_memory=True)
    test_loader = DataLoader(test_ds, batch_size=config.BATCH_SIZE * 2, shuffle=False,
                             num_workers=config.NUM_WORKERS, pin_memory=True)

    # æ¨¡åž‹åˆå§‹åŒ–
    model = BLDE_1D_Modulation(num_classes=config.NUM_CLASSES).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LR, weight_decay=config.WEIGHT_DECAY)
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

    scheduler = torch.optim.lr_scheduler.OneCycleLR(
        optimizer, max_lr=config.LR,
        steps_per_epoch=len(train_loader) // config.ACCUMULATION_STEPS,
        epochs=config.EPOCHS, pct_start=0.1
    )
    scaler = GradScaler()

    # è®­ç»ƒå¾ªçŽ¯
    best_acc = 0.0
    for epoch in range(config.EPOCHS):
        model.train()
        total_loss, correct, total = 0, 0, 0
        optimizer.zero_grad()

        pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{config.EPOCHS}", ncols=110)

        for i, (inputs, targets) in enumerate(pbar):
            inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)

            with autocast():
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                loss = loss / config.ACCUMULATION_STEPS

            scaler.scale(loss).backward()

            if (i + 1) % config.ACCUMULATION_STEPS == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
                scheduler.step()

            scaler_loss = loss.item() * config.ACCUMULATION_STEPS
            total_loss += scaler_loss
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

            pbar.set_postfix({'Loss': f"{scaler_loss:.4f}", 'Acc': f"{100. * correct / total:.2f}%"})

        train_acc = 100. * correct / total
        log_info(
            f"Epoch {epoch + 1} Train | Loss: {total_loss / len(train_loader):.4f} | Acc: {train_acc:.2f}% | {monitor_resources()}")

        val_acc = evaluate(model, val_loader, device, criterion, "Val")

        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), os.path.join(config.DATASET_OUTPUT_DIR, "best_model.pth"))
            log_info(f"âœ… Best Model Saved! Acc: {best_acc:.2f}%")

        if (epoch + 1) % config.SAVE_INTERVAL == 0:
            torch.save(model.state_dict(), os.path.join(config.DATASET_OUTPUT_DIR, f"epoch_{epoch + 1}.pth"))

    log_info("Starting Final Test...")
    model.load_state_dict(torch.load(os.path.join(config.DATASET_OUTPUT_DIR, "best_model.pth")))
    evaluate(model, test_loader, device, criterion, "Test")


def evaluate(model, loader, device, criterion, name="Val"):
    model.eval()
    total_loss, correct, total = 0, 0, 0
    with torch.no_grad():
        for inputs, targets in tqdm(loader, desc=name, ncols=100, leave=False):
            inputs, targets = inputs.to(device), targets.to(device)
            with autocast():
                outputs = model(inputs)
                loss = criterion(outputs, targets)
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

    acc = 100. * correct / total
    log_info(f"{name} Result | Loss: {total_loss / len(loader):.4f} | Acc: {acc:.2f}%")
    return acc


if __name__ == "__main__":
    torch.multiprocessing.freeze_support()
    train_model()