import os
import json
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm
import warnings

warnings.filterwarnings('ignore')

# ===================== é…ç½® =====================
class Config:
    DATASET_OUTPUT_DIR = "./modulation_dataset"
    BATCH_SIZE = 8  # 8GBæ˜¾å­˜é€‚é…
    EPOCHS = 5
    LR = 1e-4
    WEIGHT_DECAY = 1e-5
    ACCUMULATION_STEPS = 4
    # è‡ªåŠ¨è¯»å–è°ƒåˆ¶ç±»å‹æ•°
    NUM_CLASSES = len(json.load(open(os.path.join(DATASET_OUTPUT_DIR, "label_mapping.json")))['label_to_idx'])

config = Config()

# ===================== æµå¼æ•°æ®é›†åŠ è½½ç±» =====================
class StreamNpyDataset(Dataset):
    """å…¼å®¹æµå¼ç”Ÿæˆçš„npyæ•°æ®é›†"""
    def __init__(self, split='train'):
        self.data_path = os.path.join(config.DATASET_OUTPUT_DIR, f"{split}_data.npy")
        self.labels_path = os.path.join(config.DATASET_OUTPUT_DIR, f"{split}_labels.npy")
        # å†…å­˜æ˜ å°„åŠ è½½ï¼ˆé¿å…ä¸€æ¬¡æ€§åŠ è½½å¤§æ–‡ä»¶ï¼‰
        self.data = np.load(self.data_path, mmap_mode='r')
        self.labels = np.load(self.labels_path, mmap_mode='r')
        print(f"âœ… åŠ è½½{split}é›†ï¼š{len(self.data)}ä¸ªæ ·æœ¬ï¼ˆå†…å­˜æ˜ å°„æ¨¡å¼ï¼‰")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # æŒ‰éœ€è¯»å–ï¼Œä¸ç¼“å­˜
        return torch.from_numpy(self.data[idx]).float(), torch.tensor(self.labels[idx], dtype=torch.long)

# ===================== æ¨¡å‹å®šä¹‰ =====================
class Swish(nn.Module):
    def forward(self, x):
        return x * torch.sigmoid(x)

def get_activation():
    return nn.SiLU() if hasattr(nn, 'SiLU') else Swish()

class YOLO12_1D_Classifier(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.backbone = nn.Sequential(
            nn.Conv1d(2, 16, 6, 2, 3, bias=False),
            nn.BatchNorm1d(16),
            get_activation(),
            nn.Conv1d(16, 32, 3, 2, 1, bias=False),
            nn.BatchNorm1d(32),
            get_activation(),
            nn.Conv1d(32, 64, 3, 2, 1, bias=False),
            nn.BatchNorm1d(64),
            get_activation(),
            nn.Conv1d(64, 128, 3, 2, 1, bias=False),
            nn.BatchNorm1d(128),
            get_activation(),
            nn.Conv1d(128, 128, 3, 2, 1, bias=False),
            nn.BatchNorm1d(128),
            get_activation(),
        )
        self.class_head = nn.Sequential(
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(128, 64),
            nn.LayerNorm(64),
            get_activation(),
            nn.Dropout(0.1),
            nn.Linear(64, num_classes)
        )
        self.apply(lambda m: nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')
                   if isinstance(m, (nn.Conv1d, nn.Linear)) else None)
        self.to('cuda')

    def forward(self, x):
        return self.class_head(self.backbone(x.to('cuda')))

# ===================== è®­ç»ƒå‡½æ•° =====================
def train_model():
    print("\n" + "=" * 80)
    print("ğŸš€ å¼€å§‹è®­ç»ƒï¼ˆå…¼å®¹æµå¼æ•°æ®é›†ï¼‰")
    print("=" * 80)

    # æ ¡éªŒæ•°æ®é›†
    required_files = [f"{split}_data.npy" for split in ['train', 'val', 'test']] + \
                     [f"{split}_labels.npy" for split in ['train', 'val', 'test']]
    for f in required_files:
        assert os.path.exists(os.path.join(config.DATASET_OUTPUT_DIR, f)), f"âŒ æ‰¾ä¸åˆ°{f}ï¼è¯·å…ˆè¿è¡Œdataset_constructor.py"

    # Windowsé€‚é…
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"
    torch.backends.cudnn.benchmark = True
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ“Œ è®¾å¤‡ï¼š{device} ({torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'})")

    # åŠ è½½æ•°æ®é›†ï¼ˆå†…å­˜æ˜ å°„æ¨¡å¼ï¼‰
    train_dataset = StreamNpyDataset('train')
    val_dataset = StreamNpyDataset('val')
    test_dataset = StreamNpyDataset('test')

    # DataLoaderï¼ˆå¤šè¿›ç¨‹å…³é—­ï¼Œé¿å…å†…å­˜æ˜ å°„å†²çªï¼‰
    train_loader = DataLoader(
        train_dataset, batch_size=config.BATCH_SIZE, shuffle=True,
        num_workers=0, pin_memory=False, drop_last=True
    )
    val_loader = DataLoader(
        val_dataset, batch_size=config.BATCH_SIZE*2, shuffle=False,
        num_workers=0, pin_memory=False
    )
    test_loader = DataLoader(
        test_dataset, batch_size=config.BATCH_SIZE*2, shuffle=False,
        num_workers=0, pin_memory=False
    )

    # æ¨¡å‹åˆå§‹åŒ–
    model = YOLO12_1D_Classifier(config.NUM_CLASSES).to(device)
    criterion = nn.CrossEntropyLoss().to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LR, weight_decay=config.WEIGHT_DECAY)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.EPOCHS)
    scaler = GradScaler()

    # è®­ç»ƒå¾ªç¯
    best_val_acc = 0.0
    for epoch in range(config.EPOCHS):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss, train_acc, train_total = 0.0, 0.0, 0
        pbar = tqdm(train_loader, desc=f"Epoch [{epoch+1}/{config.EPOCHS}] Train")
        optimizer.zero_grad()

        for batch_idx, (data, labels) in enumerate(pbar):
            # æ˜¾å­˜æ¸…ç†
            if batch_idx % 10 == 0:
                torch.cuda.empty_cache()

            # æ··åˆç²¾åº¦è®­ç»ƒ
            with autocast():
                outputs = model(data)
                loss = criterion(outputs, labels.to(device)) / config.ACCUMULATION_STEPS

            # åå‘ä¼ æ’­
            scaler.scale(loss).backward()
            if (batch_idx + 1) % config.ACCUMULATION_STEPS == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()

            # ç»Ÿè®¡
            train_loss += loss.item() * config.ACCUMULATION_STEPS * data.size(0)
            train_acc += (outputs.argmax(1) == labels.to(device)).sum().item()
            train_total += data.size(0)
            mem_used = torch.cuda.memory_allocated(0)/1e9 if torch.cuda.is_available() else 0
            pbar.set_postfix({
                'Loss': f'{loss.item()*config.ACCUMULATION_STEPS:.4f}',
                'Acc': f'{train_acc/train_total:.4f}',
                'Mem': f'{mem_used:.1f}GB'
            })

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss, val_acc, val_total = 0.0, 0.0, 0
        with torch.no_grad():
            for data, labels in tqdm(val_loader, desc=f"Epoch [{epoch+1}/{config.EPOCHS}] Val"):
                outputs = model(data)
                loss = criterion(outputs, labels.to(device))
                val_loss += loss.item() * data.size(0)
                val_acc += (outputs.argmax(1) == labels.to(device)).sum().item()
                val_total += data.size(0)

        # ç»“æœç»Ÿè®¡
        train_loss_avg = train_loss / train_total
        train_acc_avg = train_acc / train_total
        val_loss_avg = val_loss / val_total
        val_acc_avg = val_acc / val_total
        scheduler.step()

        # æ‰“å°ç»“æœ
        print(f"\nğŸ“Š Epoch {epoch+1} ç»“æœï¼š")
        print(f"  - è®­ç»ƒæŸå¤±ï¼š{train_loss_avg:.4f} | è®­ç»ƒå‡†ç¡®ç‡ï¼š{train_acc_avg:.4f}")
        print(f"  - éªŒè¯æŸå¤±ï¼š{val_loss_avg:.4f} | éªŒè¯å‡†ç¡®ç‡ï¼š{val_acc_avg:.4f}")

        # ä¿å­˜æœ€ä¼˜æ¨¡å‹
        if val_acc_avg > best_val_acc:
            best_val_acc = val_acc_avg
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_val_acc': best_val_acc
            }, os.path.join(config.DATASET_OUTPUT_DIR, "yolo12_1d_best.pth"))
            print(f"âœ… ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆéªŒè¯å‡†ç¡®ç‡ï¼š{best_val_acc:.4f}ï¼‰")

    # æµ‹è¯•é˜¶æ®µ
    print("\nğŸ“Œ æµ‹è¯•é˜¶æ®µï¼ˆåŠ è½½æœ€ä¼˜æ¨¡å‹ï¼‰")
    checkpoint = torch.load(os.path.join(config.DATASET_OUTPUT_DIR, "yolo12_1d_best.pth"))
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    test_acc, test_total = 0.0, 0
    with torch.no_grad():
        for data, labels in tqdm(test_loader, desc="Testing"):
            outputs = model(data)
            test_acc += (outputs.argmax(1) == labels.to(device)).sum().item()
            test_total += data.size(0)
    test_acc_avg = test_acc / test_total

    # æœ€ç»ˆç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ç»ˆç»“æœï¼š")
    print(f"  - æœ€ä¼˜éªŒè¯å‡†ç¡®ç‡ï¼š{best_val_acc:.4f}")
    print(f"  - æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š{test_acc_avg:.4f}")
    print(f"  - æ¨¡å‹ä¿å­˜è‡³ï¼š{os.path.join(config.DATASET_OUTPUT_DIR, 'yolo12_1d_best.pth')}")
    print("=" * 80)

# ===================== è¿è¡Œå…¥å£ =====================
if __name__ == "__main__":
    train_model()