import os
import json
import time
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm
import warnings
import psutil
from datetime import datetime

warnings.filterwarnings('ignore')


# ===================== é…ç½®ï¼ˆé€‚é… Float16 æ•°æ®é›†ï¼‰ =====================
class Config:
    # æŒ‡å‘æ–°çš„æ•°æ®é›†ç›®å½•
    DATASET_OUTPUT_DIR = "./modulation_dataset_50overlap"

    # è®­ç»ƒæ—¥å¿—ç›®å½•
    LOG_DIR = "./train_logs"

    # === è®­ç»ƒè¶…å‚æ•° ===
    # ç”±äºä½¿ç”¨äº†Float16æ•°æ®ï¼Œæ˜¾å­˜å ç”¨æ›´å°ï¼Œå¯ä»¥å°è¯•å¢å¤§Batch Size
    BATCH_SIZE = 64  # 48Gæ˜¾å­˜æ¨è64æˆ–128
    EPOCHS = 50  # æ€»è½®æ•°
    LR = 3e-4  # åˆå§‹å­¦ä¹ ç‡ (AdamW)
    WEIGHT_DECAY = 1e-4  # æƒé‡è¡°å‡

    # æ¢¯åº¦ç´¯ç§¯ï¼šå¦‚æœæ˜¾å­˜ä¸å¤Ÿï¼Œå¢å¤§è¿™ä¸ªå€¼ï¼Œå‡å°BatchSize
    # ç°åœ¨çš„ batch=64, accum=4 ç­‰æ•ˆäº batch=256
    ACCUMULATION_STEPS = 4

    WARMUP_EPOCHS = 3  # é¢„çƒ­è½®æ•°

    # === è‡ªåŠ¨å¡«å……ï¼ˆä¸è¦æ‰‹åŠ¨æ”¹ï¼Œä¼šä»jsonè¯»å–ï¼‰ ===
    NUM_CLASSES = 0
    SAMPLE_LENGTH = 4096

    # èµ„æºä¿æŠ¤
    SAVE_INTERVAL = 5
    MAX_GPU_MEM_RATIO = 0.90


config = Config()

# åˆ›å»ºç›®å½•
os.makedirs(config.LOG_DIR, exist_ok=True)


# ===================== å·¥å…·å‡½æ•° =====================
def log_info(msg, save_to_file=True):
    """æ‰“å°å¹¶ä¿å­˜æ—¥å¿—"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_msg = f"[{timestamp}] {msg}"
    print(log_msg)
    if save_to_file:
        log_path = os.path.join(config.LOG_DIR, "train_log.txt")
        with open(log_path, 'a', encoding='utf-8') as f:
            f.write(log_msg + "\n")


def monitor_resources():
    """ç®€å•çš„èµ„æºç›‘æ§"""
    if torch.cuda.is_available():
        mem_alloc = torch.cuda.memory_allocated(0) / 1024 ** 3
        mem_res = torch.cuda.memory_reserved(0) / 1024 ** 3
        return f"GPU: {mem_alloc:.1f}/{mem_res:.1f}GB"
    return "CPU"


# ===================== æ•°æ®é›†ç±»ï¼ˆé€‚é… .npy Float16ï¼‰ =====================
class ModulationDataset(Dataset):
    def __init__(self, split='train'):
        self.split = split
        self.data_path = os.path.join(config.DATASET_OUTPUT_DIR, f"{split}_data.npy")
        self.labels_path = os.path.join(config.DATASET_OUTPUT_DIR, f"{split}_labels.npy")

        if not os.path.exists(self.data_path):
            raise FileNotFoundError(f"âŒ {split}é›†æ–‡ä»¶ç¼ºå¤±ï¼š{self.data_path}")

        # ä½¿ç”¨ mmap_mode='r' å®ç°å†…å­˜æ˜ å°„ï¼Œä¸ä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜
        # å³ä½¿æ–‡ä»¶å¾ˆå¤§ï¼Œç³»ç»Ÿå†…å­˜å ç”¨ä¹Ÿä¼šå¾ˆä½
        try:
            self.data = np.load(self.data_path, mmap_mode='r')
            self.labels = np.load(self.labels_path, mmap_mode='r')
        except Exception as e:
            raise RuntimeError(f"âŒ åŠ è½½{split}é›†å¤±è´¥ï¼š{e}")

        self.num_samples = len(self.labels)

        # æ ¡éªŒå½¢çŠ¶ [N, 2, L]
        if len(self.data.shape) != 3 or self.data.shape[1] != 2:
            log_info(f"âš ï¸ {split}é›†å½¢çŠ¶å¯èƒ½ä¸åŒ¹é…: {self.data.shape}, é¢„æœŸ [N, 2, 4096]")

        log_info(f"âœ… Loaded {split}: {self.num_samples:,} samples | Shape: {self.data.shape}")

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        try:
            # 1. è¯»å–æ•°æ® (ä»ç¡¬ç›˜/ç¼“å­˜è¯»å– Float16)
            # copy() æ˜¯ä¸ºäº†å°† mmap çš„åªè¯»æ•°æ®è½¬ä¸ºå†…å­˜ä¸­çš„å¯å†™å‰¯æœ¬ï¼Œé¿å…TorchæŠ¥é”™
            sample_np = self.data[idx].copy()
            label_val = self.labels[idx]

            # 2. è½¬æ¢ä¸º Tensor å¹¶è½¬ä¸º Float32
            # è™½ç„¶å­˜çš„æ˜¯ Float16ï¼Œä½†è¿›å…¥æ¨¡å‹é€šå¸¸éœ€è¦ Float32 (é™¤éå…¨æµç¨‹Half)
            data_tensor = torch.from_numpy(sample_np).float()
            label_tensor = torch.tensor(label_val, dtype=torch.long)

            return data_tensor, label_tensor

        except Exception as e:
            print(f"Error loading sample {idx}: {e}")
            return torch.zeros(2, config.SAMPLE_LENGTH).float(), torch.tensor(0).long()


# ===================== æ¨¡å‹å®šä¹‰ (YOLO12-1D) =====================
class Swish(nn.Module):
    def forward(self, x): return x * torch.sigmoid(x)


def get_activation():
    return nn.SiLU() if hasattr(nn, 'SiLU') else Swish()


class YOLO12_1D_Modulation(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        # Backbone: Input [B, 2, 4096]
        self.features = nn.Sequential(
            # Stage 1
            nn.Conv1d(2, 32, 7, stride=2, padding=3, bias=False),  # -> 2048
            nn.BatchNorm1d(32), get_activation(),

            # Stage 2
            nn.Conv1d(32, 64, 5, stride=2, padding=2, bias=False),  # -> 1024
            nn.BatchNorm1d(64), get_activation(),
            nn.Dropout1d(0.1),

            # Stage 3
            nn.Conv1d(64, 128, 3, stride=2, padding=1, bias=False),  # -> 512
            nn.BatchNorm1d(128), get_activation(),

            # Stage 4
            nn.Conv1d(128, 256, 3, stride=2, padding=1, bias=False),  # -> 256
            nn.BatchNorm1d(256), get_activation(),

            # Stage 5
            nn.Conv1d(256, 512, 3, stride=2, padding=1, bias=False),  # -> 128
            nn.BatchNorm1d(512), get_activation(),

            # Stage 6 (Global Context)
            nn.Conv1d(512, 512, 3, stride=2, padding=1, bias=False),  # -> 64
            nn.BatchNorm1d(512), get_activation(),
        )

        # Head
        self.avgpool = nn.AdaptiveAvgPool1d(1)
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            get_activation(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.features(x)
        x = self.avgpool(x)
        return self.classifier(x)


# ===================== è®­ç»ƒä¸»æµç¨‹ =====================
def train_model():
    log_info("=" * 60)
    log_info("ğŸš€ å¼€å§‹è®­ç»ƒ (é€‚é…æ–°æ•°æ®é›†)")
    log_info("=" * 60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    log_info(f"Using device: {device}")

    # 1. è¯»å– Label Mapping è·å–ç±»åˆ«é…ç½®
    json_path = os.path.join(config.DATASET_OUTPUT_DIR, "label_mapping.json")
    if os.path.exists(json_path):
        with open(json_path, 'r', encoding='utf-8') as f:
            mapping = json.load(f)
            # å…¼å®¹ä¸¤ç§jsonæ ¼å¼ï¼ˆä¹‹å‰ä»£ç ç”Ÿæˆçš„å’Œç›´æ¥å­—å…¸çš„ï¼‰
            if 'label_to_idx' in mapping:
                config.NUM_CLASSES = len(mapping['label_to_idx'])
            else:
                config.NUM_CLASSES = len(mapping)
        log_info(f"ğŸ“Œ è‡ªåŠ¨æ£€æµ‹ç±»åˆ«æ•°: {config.NUM_CLASSES}")
    else:
        log_info("âš ï¸ æœªæ‰¾åˆ° label_mapping.jsonï¼Œé»˜è®¤ä½¿ç”¨20ç±»")
        config.NUM_CLASSES = 20

    # 2. æ•°æ®é›†åŠ è½½
    train_ds = ModulationDataset('train')
    val_ds = ModulationDataset('val')
    test_ds = ModulationDataset('test')

    # Windowsä¸‹ num_workers å»ºè®®è®¾ä¸º 0ï¼Œé¿å…å¤šè¿›ç¨‹ä¸ mmap å†²çª
    # Linuxä¸‹ å¯ä»¥è®¾ä¸º 4 æˆ– 8
    num_workers = 0 if os.name == 'nt' else 4

    train_loader = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True,
                              num_workers=num_workers, pin_memory=True, drop_last=True)
    val_loader = DataLoader(val_ds, batch_size=config.BATCH_SIZE * 2, shuffle=False,
                            num_workers=num_workers, pin_memory=True)
    test_loader = DataLoader(test_ds, batch_size=config.BATCH_SIZE * 2, shuffle=False,
                             num_workers=num_workers, pin_memory=True)

    # 3. æ¨¡å‹åˆå§‹åŒ–
    model = YOLO12_1D_Modulation(num_classes=config.NUM_CLASSES).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LR, weight_decay=config.WEIGHT_DECAY)
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

    # ç®€å•çš„å­¦ä¹ ç‡è¡°å‡
    scheduler = torch.optim.lr_scheduler.OneCycleLR(
        optimizer, max_lr=config.LR,
        steps_per_epoch=len(train_loader) // config.ACCUMULATION_STEPS,
        epochs=config.EPOCHS, pct_start=0.1
    )

    scaler = GradScaler()  # æ··åˆç²¾åº¦è®­ç»ƒ

    # 4. å¾ªç¯
    best_acc = 0.0

    for epoch in range(config.EPOCHS):
        model.train()
        total_loss = 0
        correct = 0
        total = 0

        optimizer.zero_grad()

        pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{config.EPOCHS}", ncols=100)

        for i, (inputs, targets) in enumerate(pbar):
            inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)

            with autocast():
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                loss = loss / config.ACCUMULATION_STEPS

            scaler.scale(loss).backward()

            if (i + 1) % config.ACCUMULATION_STEPS == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
                scheduler.step()

            # ç»Ÿè®¡
            scaler_loss = loss.item() * config.ACCUMULATION_STEPS
            total_loss += scaler_loss
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

            pbar.set_postfix({'Loss': f"{scaler_loss:.4f}", 'Acc': f"{100. * correct / total:.2f}%"})

        # End of Epoch
        train_acc = 100. * correct / total
        log_info(
            f"Epoch {epoch + 1} Train | Loss: {total_loss / len(train_loader):.4f} | Acc: {train_acc:.2f}% | {monitor_resources()}")

        # Validation
        val_acc = evaluate(model, val_loader, device, criterion, "Val")

        # Save Best
        if val_acc > best_acc:
            best_acc = val_acc
            save_path = os.path.join(config.DATASET_OUTPUT_DIR, "best_model.pth")
            torch.save(model.state_dict(), save_path)
            log_info(f"âœ… New Best Model Saved! Acc: {best_acc:.2f}%")

        # Save Checkpoint
        if (epoch + 1) % config.SAVE_INTERVAL == 0:
            torch.save(model.state_dict(), os.path.join(config.DATASET_OUTPUT_DIR, f"epoch_{epoch + 1}.pth"))

    # Test
    log_info("Starting Final Test...")
    model.load_state_dict(torch.load(os.path.join(config.DATASET_OUTPUT_DIR, "best_model.pth")))
    evaluate(model, test_loader, device, criterion, "Test")


def evaluate(model, loader, device, criterion, name="Val"):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, targets in tqdm(loader, desc=name, ncols=100, leave=False):
            inputs, targets = inputs.to(device), targets.to(device)
            with autocast():
                outputs = model(inputs)
                loss = criterion(outputs, targets)

            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

    acc = 100. * correct / total
    log_info(f"{name} Result | Loss: {total_loss / len(loader):.4f} | Acc: {acc:.2f}%")
    return acc


if __name__ == "__main__":
    # Windowsä¸‹å¦‚æœæŠ¥é”™ broken pipeï¼Œä¿ç•™è¿™è¡Œï¼›å¦åˆ™åœ¨ä¸»å‡½æ•°å¤–æ‰§è¡Œ
    torch.multiprocessing.freeze_support()
    train_model()