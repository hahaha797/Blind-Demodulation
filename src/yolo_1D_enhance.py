import os
import json
import time
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm
import warnings
import psutil
from datetime import datetime

warnings.filterwarnings('ignore')


#

# ===================== é…ç½®ï¼ˆé€‚é… Float16 + 48Gå†…å­˜ï¼‰ =====================
class Config:
    # 1. è·¯å¾„ä¿®æ”¹ï¼šæŒ‡å‘ç”Ÿæˆçš„æ–°æ•°æ®é›†ç›®å½•
    DATASET_OUTPUT_DIR = "./modulation_dataset_50overlap"
    LOG_DIR = "./train_logs"

    # 2. æ€§èƒ½å‚æ•°ä¼˜åŒ–
    # ç”±äºŽæ•°æ®æ˜¯Float16ï¼Œå†…å­˜å ç”¨å‡åŠï¼Œå¯ä»¥å°†Batch Sizeå¢žå¤§ä¸€å€
    BATCH_SIZE = 64  # 48Gæ˜¾å­˜æŽ¨è64-128 (å–å†³äºŽæ¨¡åž‹å¤§å°)

    EPOCHS = 50  # æ­£å¼è®­ç»ƒè½®æ•°
    LR = 3e-4  # åˆå§‹å­¦ä¹ çŽ‡ (AdamW)
    WEIGHT_DECAY = 1e-4

    # æ¢¯åº¦ç´¯ç§¯ï¼šBatch=64 * Accum=4 => ç­‰æ•ˆ Batch=256
    ACCUMULATION_STEPS = 4
    WARMUP_EPOCHS = 3

    # 3. è‡ªåŠ¨å‚æ•°ï¼ˆåŽç»­ä»Žjsonè¯»å–ï¼‰
    NUM_CLASSES = 0
    SAMPLE_LENGTH = 4096

    # èµ„æºä¿æŠ¤
    SAVE_INTERVAL = 5
    MAX_GPU_MEM_RATIO = 0.90


config = Config()

# åˆ›å»ºæ—¥å¿—ç›®å½•
os.makedirs(config.LOG_DIR, exist_ok=True)


# ===================== å·¥å…·å‡½æ•° =====================
def log_info(msg, save_to_file=True):
    """æ‰“å°å¹¶ä¿å­˜æ—¥å¿—"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_msg = f"[{timestamp}] {msg}"
    print(log_msg)
    if save_to_file:
        log_path = os.path.join(config.LOG_DIR, "train_log.txt")
        with open(log_path, 'a', encoding='utf-8') as f:
            f.write(log_msg + "\n")


def monitor_resources():
    """èµ„æºç›‘æŽ§"""
    mem_info = "CPU"
    if torch.cuda.is_available():
        mem_alloc = torch.cuda.memory_allocated(0) / 1024 ** 3
        mem_res = torch.cuda.memory_reserved(0) / 1024 ** 3
        mem_info = f"GPU: {mem_alloc:.1f}/{mem_res:.1f}GB"

    ram_used = psutil.virtual_memory().percent
    return f"{mem_info} | RAM: {ram_used}%"


# ===================== æ•°æ®é›†ç±»ï¼ˆé€‚é… Float16 NPYï¼‰ =====================
class ModulationDataset(Dataset):
    def __init__(self, split='train'):
        self.split = split
        self.data_path = os.path.join(config.DATASET_OUTPUT_DIR, f"{split}_data.npy")
        self.labels_path = os.path.join(config.DATASET_OUTPUT_DIR, f"{split}_labels.npy")

        if not os.path.exists(self.data_path):
            raise FileNotFoundError(f"âŒ {split}é›†æ–‡ä»¶ç¼ºå¤±ï¼š{self.data_path}")

        # === æ ¸å¿ƒä¼˜åŒ–ï¼šå†…å­˜æ˜ å°„ ===
        # mmap_mode='r' æ„å‘³ç€æ•°æ®ä¿ç•™åœ¨ç¡¬ç›˜ä¸Šï¼Œéšç”¨éšå–ï¼Œä¸å ç”¨å‡ åGçš„å†…å­˜
        try:
            self.data = np.load(self.data_path, mmap_mode='r')
            self.labels = np.load(self.labels_path, mmap_mode='r')
        except Exception as e:
            raise RuntimeError(f"âŒ åŠ è½½{split}é›†å¤±è´¥ï¼š{e}")

        self.num_samples = len(self.labels)

        # æ ¡éªŒå½¢çŠ¶ [N, 2, L]
        if len(self.data.shape) != 3 or self.data.shape[1] != 2:
            log_info(f"âš ï¸ {split}é›†å½¢çŠ¶è­¦å‘Š: {self.data.shape}, é¢„æœŸ [N, 2, 4096]")

        log_info(f"âœ… Loaded {split}: {self.num_samples:,} samples | Shape: {self.data.shape}")

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        try:
            # === æ ¸å¿ƒé€‚é…ï¼šFloat16 -> Float32 ===
            # 1. ä»Ž mmap è¯»å– (Float16)
            # 2. astype(np.float32) ä¼šå°†æ•°æ®å¤åˆ¶åˆ°å†…å­˜å¹¶è½¬æ¢ç±»åž‹
            #    è¿™æ˜¯å¿…è¦çš„ï¼Œå› ä¸ºåŽç»­å·ç§¯å±‚é€šå¸¸éœ€è¦ float32 è¾“å…¥
            sample_np = self.data[idx].astype(np.float32)
            label_val = self.labels[idx]

            # 3. è½¬ä¸º Tensor
            data_tensor = torch.from_numpy(sample_np)
            label_tensor = torch.tensor(label_val, dtype=torch.long)

            return data_tensor, label_tensor

        except Exception as e:
            # å®¹é”™å¤„ç†
            return torch.zeros(2, config.SAMPLE_LENGTH).float(), torch.tensor(0).long()


# ===================== æ¨¡åž‹å®šä¹‰ (BLDE-1D) =====================
class Swish(nn.Module):
    def forward(self, x): return x * torch.sigmoid(x)


def get_activation():
    return nn.SiLU() if hasattr(nn, 'SiLU') else Swish()


class BLDE_1D_Modulation(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        # Backbone: Input [B, 2, 4096]
        # ç»“æž„æœªå˜ï¼Œä½†è¾“å…¥æ•°æ®çŽ°åœ¨æ˜¯ç²¾å‡†çš„ Float32 (ç”±Datasetç±»è½¬æ¢è€Œæ¥)
        self.features = nn.Sequential(
            # Stage 1
            nn.Conv1d(2, 32, 7, stride=2, padding=3, bias=False),  # -> 2048
            nn.BatchNorm1d(32), get_activation(),

            # Stage 2
            nn.Conv1d(32, 64, 5, stride=2, padding=2, bias=False),  # -> 1024
            nn.BatchNorm1d(64), get_activation(),
            nn.Dropout1d(0.1),

            # Stage 3
            nn.Conv1d(64, 128, 3, stride=2, padding=1, bias=False),  # -> 512
            nn.BatchNorm1d(128), get_activation(),

            # Stage 4
            nn.Conv1d(128, 256, 3, stride=2, padding=1, bias=False),  # -> 256
            nn.BatchNorm1d(256), get_activation(),

            # Stage 5
            nn.Conv1d(256, 512, 3, stride=2, padding=1, bias=False),  # -> 128
            nn.BatchNorm1d(512), get_activation(),

            # Stage 6 (Global Context)
            nn.Conv1d(512, 512, 3, stride=2, padding=1, bias=False),  # -> 64
            nn.BatchNorm1d(512), get_activation(),
        )

        # Head
        self.avgpool = nn.AdaptiveAvgPool1d(1)
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            get_activation(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.features(x)
        x = self.avgpool(x)
        return self.classifier(x)


# ===================== è®­ç»ƒä¸»æµç¨‹ =====================
def train_model():
    log_info("=" * 60)
    log_info("ðŸš€ å¼€å§‹è®­ç»ƒ (é€‚é… Float16 æ•°æ®é›†)")
    log_info("=" * 60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    log_info(f"Using device: {device}")

    # 1. è‡ªåŠ¨è¯»å– Label Mapping èŽ·å–ç±»åˆ«é…ç½®
    json_path = os.path.join(config.DATASET_OUTPUT_DIR, "label_mapping.json")
    if os.path.exists(json_path):
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                mapping = json.load(f)
                # å…¼å®¹ä¸¤ç§jsonæ ¼å¼ï¼ˆä¹‹å‰ä»£ç ç”Ÿæˆçš„å’Œç›´æŽ¥å­—å…¸çš„ï¼‰
                if 'label_to_idx' in mapping:  # æ—§æ ¼å¼
                    config.NUM_CLASSES = len(mapping['label_to_idx'])
                elif isinstance(mapping, dict):  # æ–°æ ¼å¼ (ç›´æŽ¥æ˜¯å­—å…¸)
                    config.NUM_CLASSES = len(mapping)
            log_info(f"ðŸ“Œ è‡ªåŠ¨æ£€æµ‹ç±»åˆ«æ•°: {config.NUM_CLASSES}")
        except Exception as e:
            log_info(f"âš ï¸ è¯»å–jsonå¤±è´¥: {e}, é»˜è®¤ä½¿ç”¨20ç±»")
            config.NUM_CLASSES = 20
    else:
        log_info("âš ï¸ æœªæ‰¾åˆ° label_mapping.jsonï¼Œé»˜è®¤ä½¿ç”¨20ç±»")
        config.NUM_CLASSES = 20

    # 2. æ•°æ®é›†åŠ è½½
    train_ds = ModulationDataset('train')
    val_ds = ModulationDataset('val')
    test_ds = ModulationDataset('test')

    # Windowsä¸‹ num_workers å¿…é¡»è®¾ä¸º 0ï¼Œå¦åˆ™ mmap æ–‡ä»¶å¥æŸ„ä¼šæŠ¥é”™
    num_workers = 0 if os.name == 'nt' else 4

    train_loader = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True,
                              num_workers=num_workers, pin_memory=True, drop_last=True)
    val_loader = DataLoader(val_ds, batch_size=config.BATCH_SIZE * 2, shuffle=False,
                            num_workers=num_workers, pin_memory=True)
    test_loader = DataLoader(test_ds, batch_size=config.BATCH_SIZE * 2, shuffle=False,
                             num_workers=num_workers, pin_memory=True)

    # 3. æ¨¡åž‹åˆå§‹åŒ–
    model = BLDE_1D_Modulation(num_classes=config.NUM_CLASSES).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LR, weight_decay=config.WEIGHT_DECAY)
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

    # å­¦ä¹ çŽ‡ç­–ç•¥ (OneCycle æ•ˆæžœé€šå¸¸æœ€å¥½)
    scheduler = torch.optim.lr_scheduler.OneCycleLR(
        optimizer, max_lr=config.LR,
        steps_per_epoch=len(train_loader) // config.ACCUMULATION_STEPS,
        epochs=config.EPOCHS, pct_start=0.1
    )

    scaler = GradScaler()  # æ··åˆç²¾åº¦è®­ç»ƒå·¥å…·

    # 4. è®­ç»ƒå¾ªçŽ¯
    best_acc = 0.0

    for epoch in range(config.EPOCHS):
        model.train()
        total_loss = 0
        correct = 0
        total = 0

        optimizer.zero_grad()

        pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{config.EPOCHS}", ncols=110)

        for i, (inputs, targets) in enumerate(pbar):
            # non_blocking=True åŠ é€Ÿæ•°æ®ä¼ è¾“
            inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)

            # æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡
            with autocast():
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                loss = loss / config.ACCUMULATION_STEPS

            # åå‘ä¼ æ’­ (ç¼©æ”¾æ¢¯åº¦)
            scaler.scale(loss).backward()

            # æ¢¯åº¦ç´¯ç§¯æ›´æ–°
            if (i + 1) % config.ACCUMULATION_STEPS == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
                scheduler.step()

            # ç»Ÿè®¡
            scaler_loss = loss.item() * config.ACCUMULATION_STEPS
            total_loss += scaler_loss
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

            pbar.set_postfix({'Loss': f"{scaler_loss:.4f}", 'Acc': f"{100. * correct / total:.2f}%"})

        # End of Epoch
        train_acc = 100. * correct / total
        log_info(
            f"Epoch {epoch + 1} Train | Loss: {total_loss / len(train_loader):.4f} | Acc: {train_acc:.2f}% | {monitor_resources()}")

        # Validation
        val_acc = evaluate(model, val_loader, device, criterion, "Val")

        # Save Best
        if val_acc > best_acc:
            best_acc = val_acc
            save_path = os.path.join(config.DATASET_OUTPUT_DIR, "best_model.pth")
            torch.save(model.state_dict(), save_path)
            log_info(f"âœ… New Best Model Saved! Acc: {best_acc:.2f}%")

        # Save Checkpoint
        if (epoch + 1) % config.SAVE_INTERVAL == 0:
            torch.save(model.state_dict(), os.path.join(config.DATASET_OUTPUT_DIR, f"epoch_{epoch + 1}.pth"))

    # Test
    log_info("Starting Final Test...")
    model.load_state_dict(torch.load(os.path.join(config.DATASET_OUTPUT_DIR, "best_model.pth")))
    evaluate(model, test_loader, device, criterion, "Test")


def evaluate(model, loader, device, criterion, name="Val"):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, targets in tqdm(loader, desc=name, ncols=100, leave=False):
            inputs, targets = inputs.to(device), targets.to(device)
            with autocast():
                outputs = model(inputs)
                loss = criterion(outputs, targets)

            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

    acc = 100. * correct / total
    log_info(f"{name} Result | Loss: {total_loss / len(loader):.4f} | Acc: {acc:.2f}%")
    return acc


if __name__ == "__main__":
    # Windowsä¸‹é˜²æ­¢å¤šè¿›ç¨‹æŠ¥é”™
    torch.multiprocessing.freeze_support()
    train_model()