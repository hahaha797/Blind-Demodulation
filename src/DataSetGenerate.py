import os
import re
import json
import time
import psutil
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import warnings
import shutil

warnings.filterwarnings('ignore')


# ===================== é…ç½®ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ =====================
class Config:
    # æ•°æ®è·¯å¾„é…ç½®
    DATA_DIR = "../../DataSet"  # åŸå§‹.bin/.wavæ–‡ä»¶ç›®å½•
    DATASET_OUTPUT_DIR = "./modulation_dataset_50overlap"  # æ•°æ®é›†è¾“å‡ºç›®å½•

    SAMPLE_LENGTH = 4096  # å•æ ·æœ¬IQé•¿åº¦

    # === æ ¸å¿ƒä¼˜åŒ– 1ï¼šæ­¥é•¿è®¾ç½® (50% é‡å ) ===
    # æ­¥é•¿ = 2048ï¼Œå³æ¯æ¬¡ç§»åŠ¨åŠä¸ªçª—å£ï¼Œæ—¢å¢åŠ äº†æ•°æ®é‡åˆé¿å…äº†è¿‡åº¦è†¨èƒ€
    STRIDE = 2048

    # === æ ¸å¿ƒä¼˜åŒ– 2ï¼šæ•°æ®ç²¾åº¦ ===
    # ä½¿ç”¨ float16 æ›¿ä»£ float32ï¼Œä½“ç§¯å‡åŠï¼Œæ˜¾å­˜å ç”¨å‡åŠ
    DTYPE = np.float16

    # æµå¼åˆ†å—é…ç½®
    CHUNK_SIZE = 100000  # æ¯å—æ ·æœ¬æ•°

    # æ•°æ®é›†åˆ’åˆ†
    TEST_SIZE = 0.1
    VAL_SIZE = 0.111
    RANDOM_STATE = 42

    # æ‰“å°/ç›‘æ§é…ç½®
    PRINT_MEMORY_USAGE = True
    PRINT_SAMPLE_DISTRIBUTION = True


config = Config()

# åˆ›å»ºç›®å½•
os.makedirs(config.DATASET_OUTPUT_DIR, exist_ok=True)
temp_chunk_dir = os.path.join(config.DATASET_OUTPUT_DIR, "temp_chunks")
os.makedirs(temp_chunk_dir, exist_ok=True)


# ===================== è¾…åŠ©å‡½æ•° =====================
def print_memory_usage(step_name=""):
    """æ‰“å°å½“å‰å†…å­˜å ç”¨"""
    if not Config.PRINT_MEMORY_USAGE:
        return
    process = psutil.Process(os.getpid())
    mem_usage = process.memory_info().rss / 1024 / 1024 / 1024  # GB
    print(f"ğŸ“Š å†…å­˜å ç”¨ [{step_name}]ï¼š{mem_usage:.2f} GB")


def print_sample_distribution(modulation_samples, label_encoder_mapping):
    """æ‰“å°å„è°ƒåˆ¶ç±»å‹æ ·æœ¬åˆ†å¸ƒ"""
    if not Config.PRINT_SAMPLE_DISTRIBUTION:
        return
    print("\n" + "-" * 70)
    print("ğŸ“ˆ å„è°ƒåˆ¶ç±»å‹æ ·æœ¬åˆ†å¸ƒæ±‡æ€» (æ­¥é•¿: {} | 50%é‡å )".format(config.STRIDE))
    print(f"{'è°ƒåˆ¶ç±»å‹':<12} {'æ ·æœ¬æ€»æ•°':<15} {'æ ‡ç­¾ID':<8} {'æ–‡ä»¶æ•°':<8}")
    print("-" * 70)
    total_all = 0
    for mod, items in modulation_samples.items():
        mod_total = sum([item['file_info']['estimated_samples'] for item in items])
        total_all += mod_total
        label_id = label_encoder_mapping.get(mod, -1)
        file_count = len(items)
        print(f"{mod:<12} {mod_total:<15,} {label_id:<8} {file_count:<8}")
    print("-" * 70)
    print(f"{'æ€»è®¡':<12} {total_all:<15,} {'-':<8} {len([i for v in modulation_samples.values() for i in v]):<8}")
    print("-" * 70)


# ===================== æ ¸å¿ƒå‡½æ•°ï¼šæ–‡ä»¶è§£æ =====================
def get_file_iq_info(file_path):
    """
    è§£æå•ä¸ªæ–‡ä»¶çš„IQåŸºç¡€ä¿¡æ¯ï¼Œå¹¶æ ¹æ®æ­¥é•¿ä¼°ç®—æ ·æœ¬æ•°
    """
    try:
        filename = os.path.basename(file_path)
        file_ext = os.path.splitext(filename)[1].lower()
        file_info = {
            'file_path': file_path,
            'filename': filename,
            'file_type': file_ext[1:],  # bin/wav
            'modulation': None,
            'sample_rate_hz': None,
            'total_iq_pairs': 0,
            'valid_iq_pairs': 0,
            'estimated_samples': 0,  # æ ¹æ®æ­¥é•¿è®¡ç®—çš„æ ·æœ¬æ•°
            'frame_size': None,
            'total_frames': None
        }

        # æå–è°ƒåˆ¶ç±»å‹
        name_without_ext = os.path.splitext(filename)[0]
        modulation_part = name_without_ext.split('_', 1)[0].strip()
        file_info['modulation'] = modulation_part

        # æå–é‡‡æ ·ç‡
        sample_rate_pattern = r'(\d+\.?\d*)\s*([kM]SPS)'
        sample_rate_match = re.search(sample_rate_pattern, name_without_ext)
        if sample_rate_match:
            num = float(sample_rate_match.group(1))
            unit = sample_rate_match.group(2)
            sample_rate = num * 1e3 if unit == 'kSPS' else num * 1e6
            file_info['sample_rate_hz'] = sample_rate

        # ========== BINæ–‡ä»¶è§£æ ==========
        if file_ext == '.bin':
            frame_size_pattern = r'_(\d+)\.bin$'
            frame_size_match = re.search(frame_size_pattern, filename)
            if not frame_size_match:
                return None

            frame_size = int(frame_size_match.group(1))
            file_info['frame_size'] = frame_size

            # ä»…ä»…è¯»å–å…ƒæ•°æ®ï¼Œä¸è¯»å–æ•´ä¸ªæ–‡ä»¶å†…å®¹
            file_size_bytes = os.path.getsize(file_path)
            total_iq = file_size_bytes // 4  # int16 * 2 = 4 bytes

            file_info['total_iq_pairs'] = total_iq
            total_frames = total_iq // frame_size
            file_info['total_frames'] = total_frames
            file_info['valid_iq_pairs'] = total_frames * frame_size

            # è®¡ç®—æ ·æœ¬æ•° (é€‚é… Stride)
            # æ¯å¸§çš„æœ‰æ•ˆåŒºé—´é•¿åº¦
            valid_len_per_frame = frame_size - config.SAMPLE_LENGTH + 1
            if valid_len_per_frame <= 0:
                return None

            # æ¯å¸§èƒ½åˆ‡å‡ºçš„æ ·æœ¬æ•° = (æœ‰æ•ˆé•¿åº¦ / æ­¥é•¿) å‘ä¸Šå–æ•´
            samples_per_frame = (valid_len_per_frame + config.STRIDE - 1) // config.STRIDE
            file_info['estimated_samples'] = total_frames * samples_per_frame

        # ========== WAVæ–‡ä»¶è§£æ ==========
        elif file_ext == '.wav':
            file_size = os.path.getsize(file_path)
            # è·³è¿‡1068å­—èŠ‚å¤´ï¼Œå‰©ä½™å­—èŠ‚ / 4 = IQå¯¹æ•°
            valid_bytes = file_size - 1068
            if valid_bytes <= 0: return None

            total_iq = valid_bytes // 4
            file_info['total_iq_pairs'] = total_iq
            file_info['valid_iq_pairs'] = total_iq

            # è®¡ç®—æ ·æœ¬æ•° (é€‚é… Stride)
            if total_iq < config.SAMPLE_LENGTH:
                return None

            valid_len = total_iq - config.SAMPLE_LENGTH + 1
            # æ ·æœ¬æ•° = (æœ‰æ•ˆé•¿åº¦ / æ­¥é•¿) å‘ä¸Šå–æ•´
            file_info['estimated_samples'] = (valid_len + config.STRIDE - 1) // config.STRIDE

        else:
            return None

        if file_info['estimated_samples'] <= 0:
            return None

        print(f"   â””â”€ {filename} -> é¢„è®¡ç”Ÿæˆæ ·æœ¬: {file_info['estimated_samples']:,} (æ­¥é•¿: {config.STRIDE})")
        return file_info

    except Exception as e:
        print(f"âš ï¸  å¤„ç†æ–‡ä»¶å¤±è´¥ï¼š{file_path} -> {str(e)}")
        return None


def stream_read_sample(file_info, start_idx):
    """
    è¯»å–æ ·æœ¬å¹¶è½¬ä¸º Float16
    """
    file_path = file_info['file_path'].replace('/', '\\')
    if not os.path.exists(file_path):
        file_path = os.path.join(config.DATA_DIR, os.path.basename(file_path))

    if file_info['file_type'] == 'bin':
        seek_pos = start_idx * 4
    elif file_info['file_type'] == 'wav':
        seek_pos = 1068 + start_idx * 4
    else:
        return np.zeros((config.SAMPLE_LENGTH, 2), dtype=config.DTYPE)

    try:
        with open(file_path, 'rb') as f:
            f.seek(seek_pos)
            data = np.fromfile(f, dtype=np.int16, count=config.SAMPLE_LENGTH * 2)
    except Exception as e:
        return np.zeros((config.SAMPLE_LENGTH, 2), dtype=config.DTYPE)

    if len(data) < config.SAMPLE_LENGTH * 2:
        sample = np.zeros((config.SAMPLE_LENGTH, 2), dtype=np.int16)
        valid_len = len(data) // 2
        sample[:valid_len] = data.reshape(-1, 2)
    else:
        sample = data.reshape(-1, 2)

    # å½’ä¸€åŒ–å¹¶è½¬ä¸º float16
    sample_norm = sample.astype(config.DTYPE) / 32767.0
    return sample_norm


# ===================== ä¸»å‡½æ•° =====================
def construct_dataset_stream():
    total_start_time = time.time()

    print("=" * 80)
    print("ğŸš€ å¼€å§‹æµå¼æ„é€ è°ƒåˆ¶ä¿¡å·æ•°æ®é›† (Float16 + 50% Overlap)")
    print(f"âš™ï¸  é…ç½®ï¼šæ ·æœ¬é•¿={config.SAMPLE_LENGTH} | æ­¥é•¿={config.STRIDE} | ç²¾åº¦={config.DTYPE.__name__}")
    print("=" * 80)

    # 1. ç»Ÿè®¡æ–‡ä»¶
    print("\nğŸ“Œ ç¬¬ä¸€æ­¥ï¼šè§£ææ–‡ä»¶ä¿¡æ¯...")
    all_file_metadata = []
    modulation_samples = {}
    total_samples_estimated = 0

    for filename in os.listdir(config.DATA_DIR):
        file_path = os.path.join(config.DATA_DIR, filename)
        if not os.path.isfile(file_path): continue

        file_info = get_file_iq_info(file_path)
        if not file_info: continue

        all_file_metadata.append(file_info)
        mod = file_info['modulation']
        if mod not in modulation_samples: modulation_samples[mod] = []

        modulation_samples[mod].append({'file_info': file_info})
        total_samples_estimated += file_info['estimated_samples']

    # ä¿å­˜å…ƒæ•°æ®å’Œæ ‡ç­¾æ˜ å°„
    all_modulations = sorted(list(modulation_samples.keys()))
    label_encoder_mapping = {mod: idx for idx, mod in enumerate(all_modulations)}

    # æ‰“å°åˆ†å¸ƒ
    print_sample_distribution(modulation_samples, label_encoder_mapping)

    # ä¿å­˜ Label Mapping
    label_path = os.path.join(config.DATASET_OUTPUT_DIR, "label_mapping.json")
    with open(label_path, 'w', encoding='utf-8') as f:
        json.dump(label_encoder_mapping, f, ensure_ascii=False, indent=4)

    # 2. ç”Ÿæˆç´¢å¼•å¹¶åˆ†å—
    print("\nğŸ“Œ ç¬¬äºŒæ­¥ï¼šç”Ÿæˆç´¢å¼•ã€åˆ’åˆ†å¹¶åˆ†å—å†™å…¥...")
    chunk_counter = {'train': 0, 'val': 0, 'test': 0}
    buffer = {
        'train': {'data': [], 'labels': []},
        'val': {'data': [], 'labels': []},
        'test': {'data': [], 'labels': []}
    }

    for modulation in tqdm(all_modulations, desc="å¤„ç†è°ƒåˆ¶ç±»å‹"):
        mod_label = label_encoder_mapping[modulation]
        mod_files = modulation_samples[modulation]

        # æ”¶é›†è¯¥è°ƒåˆ¶ç±»å‹ä¸‹çš„æ‰€æœ‰æ ·æœ¬ç´¢å¼•
        mod_all_samples = []

        for file_item in mod_files:
            file_info = file_item['file_info']

            if file_info['file_type'] == 'bin':
                # BINæ–‡ä»¶ï¼šæŒ‰å¸§ + æ­¥é•¿ éå†
                frame_size = file_info['frame_size']
                total_frames = file_info['total_frames']

                # æ¯ä¸€å¸§çš„æœ‰æ•ˆèµ·å§‹ç‚¹èŒƒå›´
                max_start_in_frame = frame_size - config.SAMPLE_LENGTH + 1

                frame_start_addr = 0
                for _ in range(total_frames):
                    # åœ¨å¸§å†…åº”ç”¨æ­¥é•¿
                    for inner_start in range(0, max_start_in_frame, config.STRIDE):
                        global_start = frame_start_addr + inner_start
                        mod_all_samples.append((file_info, global_start))
                    frame_start_addr += frame_size

            else:
                # WAVæ–‡ä»¶ï¼šå…¨ç¨‹åº”ç”¨æ­¥é•¿
                max_start = file_info['valid_iq_pairs'] - config.SAMPLE_LENGTH + 1
                for start_idx in range(0, max_start, config.STRIDE):
                    mod_all_samples.append((file_info, start_idx))

        # åˆ’åˆ†æ•°æ®é›† (Train/Val/Test)
        if len(mod_all_samples) == 0: continue

        X_train_val, X_test = train_test_split(mod_all_samples, test_size=config.TEST_SIZE,
                                               random_state=config.RANDOM_STATE)
        X_train, X_val = train_test_split(X_train_val, test_size=config.VAL_SIZE, random_state=config.RANDOM_STATE)

        # å®šä¹‰å†™å…¥å‡½æ•°
        def process_split(sample_list, split_name):
            for item in sample_list:
                f_info, s_idx = item
                data = stream_read_sample(f_info, s_idx)

                buffer[split_name]['data'].append(data)
                buffer[split_name]['labels'].append(mod_label)

                # ç¼“å†²åŒºæ»¡ -> å†™å…¥
                if len(buffer[split_name]['data']) >= config.CHUNK_SIZE:
                    save_chunk(split_name)

        def save_chunk(split_name):
            data_arr = np.array(buffer[split_name]['data']).transpose(0, 2, 1)  # (N, 2, L)
            label_arr = np.array(buffer[split_name]['labels'])

            c_idx = chunk_counter[split_name]
            path = os.path.join(temp_chunk_dir, f"{split_name}_chunk_{c_idx}.npz")

            # ä½¿ç”¨å‹ç¼©ä¿å­˜ä»¥è¿›ä¸€æ­¥å‡å°ä½“ç§¯
            np.savez_compressed(path, data=data_arr, labels=label_arr)

            buffer[split_name]['data'] = []
            buffer[split_name]['labels'] = []
            chunk_counter[split_name] += 1
            print_memory_usage(f"{split_name}_chunk_{c_idx}")

        # æ‰§è¡Œå†™å…¥
        process_split(X_train, 'train')
        process_split(X_val, 'val')
        process_split(X_test, 'test')

    # 3. å†™å…¥å‰©ä½™æ•°æ®
    print("\nğŸ“Œ ç¬¬ä¸‰æ­¥ï¼šæ¸…ç†ç¼“å†²åŒº...")
    for split in ['train', 'val', 'test']:
        if len(buffer[split]['data']) > 0:
            save_chunk(split)

    # 4. åˆå¹¶åˆ†å— (ä¿®å¤äº† Windows PermissionError é—®é¢˜)
    print("\nğŸ“Œ ç¬¬å››æ­¥ï¼šåˆå¹¶æœ€ç»ˆæ–‡ä»¶...")
    for split in ['train', 'val', 'test']:
        all_data = []
        all_labels = []
        count = chunk_counter[split]

        if count == 0: continue

        print(f"åˆå¹¶ {split} é›† ({count} ä¸ªåˆ†å—)...")
        for i in tqdm(range(count)):
            p = os.path.join(temp_chunk_dir, f"{split}_chunk_{i}.npz")
            if os.path.exists(p):
                # ================= å…³é”®ä¿®å¤ =================
                # ä½¿ç”¨ with ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç¡®ä¿åŠ è½½å®Œè‡ªåŠ¨å…³é—­æ–‡ä»¶å¥æŸ„
                try:
                    with np.load(p) as loaded:
                        all_data.append(loaded['data'])
                        all_labels.append(loaded['labels'])

                    # æ­¤æ—¶æ–‡ä»¶å·²å…³é—­ï¼Œå¯ä»¥å®‰å…¨åˆ é™¤
                    os.remove(p)
                except Exception as e:
                    print(f"âš ï¸  è­¦å‘Šï¼šæ— æ³•åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {p} -> {e}")
                # ===========================================

        if all_data:
            final_data = np.concatenate(all_data, axis=0)
            final_labels = np.concatenate(all_labels, axis=0)

            # ä¿å­˜æœ€ç»ˆæ–‡ä»¶
            np.save(os.path.join(config.DATASET_OUTPUT_DIR, f"{split}_data.npy"), final_data)
            np.save(os.path.join(config.DATASET_OUTPUT_DIR, f"{split}_labels.npy"), final_labels)

            data_size = final_data.nbytes / 1024 / 1024 / 1024
            print(f"âœ… {split} ä¿å­˜å®Œæˆ: æ ·æœ¬æ•° {len(final_data):,} | å¤§å° {data_size:.2f} GB")

    # åˆ é™¤ä¸´æ—¶ç›®å½•
    try:
        if os.path.exists(temp_chunk_dir):
            shutil.rmtree(temp_chunk_dir)  # å¼ºåŠ›åˆ é™¤æ•´ä¸ªæ–‡ä»¶å¤¹
    except Exception as e:
        print(f"âš ï¸  æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")

    print("\n" + "=" * 80)
    print(f"ğŸ‰ å¤„ç†å®Œæˆï¼è¾“å‡ºç›®å½•ï¼š{config.DATASET_OUTPUT_DIR}")
    print(f"ğŸš€ æœ€ç»ˆæ­¥é•¿ï¼š{config.STRIDE} (50% Overlap)")
    print(f"ğŸ’¾ æ•°æ®ç±»å‹ï¼š{config.DTYPE.__name__} (Float16)")
    print("=" * 80)


if __name__ == "__main__":
    construct_dataset_stream()