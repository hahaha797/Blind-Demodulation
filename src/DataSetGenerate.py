import os
import re
import json
import time
import psutil
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import warnings

warnings.filterwarnings('ignore')


# ===================== é…ç½®ï¼ˆä»…ä¿®æ”¹æ­¤å¤„ï¼‰ =====================
class Config:
    # æ•°æ®è·¯å¾„é…ç½®
    DATA_DIR = "../../DataSet"  # åŸå§‹.bin/.wavæ–‡ä»¶ç›®å½•
    DATASET_OUTPUT_DIR = "./modulation_dataset"  # æ•°æ®é›†è¾“å‡ºç›®å½•
    SAMPLE_LENGTH = 4096  # å•æ ·æœ¬IQé•¿åº¦ï¼ˆå›ºå®šï¼‰

    # æµå¼åˆ†å—é…ç½®ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰
    CHUNK_SIZE = 100000  # æ¯å—æ ·æœ¬æ•°ï¼ˆ10ä¸‡/å—ï¼Œå¯æ ¹æ®å†…å­˜è°ƒæ•´ï¼‰

    # æ•°æ®é›†åˆ’åˆ†
    TEST_SIZE = 0.1  # æµ‹è¯•é›†æ¯”ä¾‹
    VAL_SIZE = 0.111  # éªŒè¯é›†æ¯”ä¾‹ï¼ˆç›¸å¯¹äºtrain_valï¼‰
    RANDOM_STATE = 42  # éšæœºç§å­

    # æ‰“å°/ç›‘æ§é…ç½®
    PRINT_MEMORY_USAGE = True  # æ˜¯å¦æ‰“å°å†…å­˜å ç”¨
    PRINT_SAMPLE_DISTRIBUTION = True  # æ˜¯å¦æ‰“å°æ ·æœ¬åˆ†å¸ƒ


config = Config()

# åˆ›å»ºç›®å½•
os.makedirs(config.DATASET_OUTPUT_DIR, exist_ok=True)
temp_chunk_dir = os.path.join(config.DATASET_OUTPUT_DIR, "temp_chunks")
os.makedirs(temp_chunk_dir, exist_ok=True)


# ===================== è¾…åŠ©å‡½æ•°ï¼šè¿›åº¦/å†…å­˜ç›‘æ§ =====================
def print_memory_usage(step_name=""):
    """æ‰“å°å½“å‰å†…å­˜å ç”¨"""
    if not Config.PRINT_MEMORY_USAGE:
        return
    process = psutil.Process(os.getpid())
    mem_usage = process.memory_info().rss / 1024 / 1024 / 1024  # è½¬æ¢ä¸ºGB
    mem_percent = process.memory_percent()
    print(f"ğŸ“Š å†…å­˜å ç”¨ [{step_name}]ï¼š{mem_usage:.2f} GB | å æ¯”ï¼š{mem_percent:.1f}%")


def print_sample_distribution(modulation_samples, label_encoder_mapping):
    """æ‰“å°å„è°ƒåˆ¶ç±»å‹æ ·æœ¬åˆ†å¸ƒæ±‡æ€»"""
    if not Config.PRINT_SAMPLE_DISTRIBUTION:
        return
    print("\n" + "-" * 70)
    print("ğŸ“ˆ å„è°ƒåˆ¶ç±»å‹æ ·æœ¬åˆ†å¸ƒæ±‡æ€»ï¼š")
    print(f"{'è°ƒåˆ¶ç±»å‹':<12} {'æ ·æœ¬æ€»æ•°':<15} {'æ ‡ç­¾ID':<8} {'æ–‡ä»¶æ•°':<8}")
    print("-" * 70)
    total_all = 0
    for mod, items in modulation_samples.items():
        mod_total = sum([item['file_info']['num_samples'] for item in items])
        total_all += mod_total
        label_id = label_encoder_mapping.get(mod, -1)
        file_count = len(items)
        print(f"{mod:<12} {mod_total:<15,} {label_id:<8} {file_count:<8}")
    print("-" * 70)
    print(f"{'æ€»è®¡':<12} {total_all:<15,} {'-':<8} {len([i for v in modulation_samples.values() for i in v]):<8}")
    print("-" * 70)


# ===================== æ ¸å¿ƒå‡½æ•°ï¼šæ–‡ä»¶è§£æ+æ ·æœ¬è¯»å– =====================
def get_file_iq_info(file_path):
    """
    è§£æå•ä¸ªæ–‡ä»¶çš„IQåŸºç¡€ä¿¡æ¯
    é€‚é…è§„èŒƒï¼š
    - BINï¼šæ— æ–‡ä»¶å¤´ï¼Œä»æ–‡ä»¶åæå–å•å¸§IQå¯¹æ•°ï¼ˆæœ€åä¸€ä¸ªä¸‹åˆ’çº¿åã€.binå‰çš„æ•°å­—ï¼‰
    - WAVï¼šè·³è¿‡1068å­—èŠ‚å¤´ï¼Œæ— å¸§æ¦‚å¿µï¼Œå…¨è¿ç»­IQ
    """
    try:
        filename = os.path.basename(file_path)
        file_ext = os.path.splitext(filename)[1].lower()
        file_info = {
            'file_path': file_path,
            'filename': filename,
            'file_type': file_ext[1:],  # bin/wav
            'modulation': None,
            'sample_rate_hz': None,
            'total_iq_pairs': 0,
            'valid_iq_pairs': 0,
            'num_samples': 0,
            'sample_length': config.SAMPLE_LENGTH,
            'frame_size': None,  # ä»…BINæ–‡ä»¶æœ‰æ•ˆ
            'total_frames': None  # ä»…BINæ–‡ä»¶æœ‰æ•ˆ
        }

        # ========== æå–è°ƒåˆ¶ç±»å‹ï¼ˆå…¼å®¹ç‰¹æ®Šå­—ç¬¦å¦‚Ï€-4DQPSKï¼‰ ==========
        name_without_ext = os.path.splitext(filename)[0]
        # åˆ†å‰²è°ƒåˆ¶ç±»å‹ï¼šç¬¬ä¸€ä¸ªä¸‹åˆ’çº¿å‰çš„æ‰€æœ‰å­—ç¬¦ï¼ˆåŒ…å«ç‰¹æ®Šå­—ç¬¦å¦‚Ï€ã€-ï¼‰
        modulation_part = name_without_ext.split('_', 1)[0].strip()  # åªåˆ†å‰²ç¬¬ä¸€ä¸ªä¸‹åˆ’çº¿
        file_info['modulation'] = modulation_part

        # ========== æå–é‡‡æ ·ç‡ ==========
        # åŒ¹é…é‡‡æ ·ç‡ï¼ˆå¦‚200kSPSã€1.953MSPSã€61.03515625kSPSï¼‰
        sample_rate_pattern = r'(\d+\.?\d*)\s*([kM]SPS)'
        sample_rate_match = re.search(sample_rate_pattern, name_without_ext)
        if sample_rate_match:
            num = float(sample_rate_match.group(1))
            unit = sample_rate_match.group(2)
            sample_rate = num * 1e3 if unit == 'kSPS' else num * 1e6
            file_info['sample_rate_hz'] = sample_rate

        # ========== BINæ–‡ä»¶è§£æï¼ˆä¿®å¤ï¼šæå–å¸§å¤§å°ï¼‰ ==========
        if file_ext == '.bin':
            # ä¿®å¤æ­£åˆ™ï¼šåŒ¹é…æœ€åä¸€ä¸ªä¸‹åˆ’çº¿åçš„æ•°å­—ï¼ˆæ ¼å¼ï¼šxxx_xxx_131072.bin â†’ 131072ï¼‰
            frame_size_pattern = r'_(\d+)\.bin$'  # åŒ¹é….binå‰çš„æ•°å­—ï¼Œä¸”å‰é¢æœ‰ä¸‹åˆ’çº¿
            frame_size_match = re.search(frame_size_pattern, filename)

            if not frame_size_match:
                print(f"âš ï¸  BINæ–‡ä»¶{filename}å‘½åä¸è§„èŒƒï¼Œæ— æ³•æå–å¸§å¤§å°ï¼ˆæ ¼å¼åº”ä¸ºï¼šè°ƒåˆ¶ç±»å‹_é‡‡æ ·ç‡_å¸§å¤§å°.binï¼‰ï¼Œè·³è¿‡")
                return None

            frame_size = int(frame_size_match.group(1))
            file_info['frame_size'] = frame_size

            # è¯»å–æ–‡ä»¶å¹¶è®¡ç®—IQå¯¹
            with open(file_path, 'rb') as f:
                data = np.fromfile(f, dtype=np.int16)
            total_iq = len(data) // 2  # æ€»IQå¯¹æ•°é‡
            file_info['total_iq_pairs'] = total_iq

            # è®¡ç®—æœ‰æ•ˆå¸§å’Œæœ‰æ•ˆIQå¯¹
            total_frames = total_iq // frame_size
            valid_iq = total_frames * frame_size
            file_info['total_frames'] = total_frames
            file_info['valid_iq_pairs'] = valid_iq

            # è®¡ç®—æ ·æœ¬æ•°ï¼šæ¯å¸§å¯ç”Ÿæˆ frame_size - SAMPLE_LENGTH + 1 ä¸ªæ ·æœ¬
            samples_per_frame = frame_size - config.SAMPLE_LENGTH + 1
            if samples_per_frame <= 0:
                print(f"âš ï¸  BINæ–‡ä»¶{filename}å¸§å¤§å°{frame_size} < æ ·æœ¬é•¿åº¦{config.SAMPLE_LENGTH}ï¼Œæ— æœ‰æ•ˆæ ·æœ¬")
                return None
            num_samples = total_frames * samples_per_frame
            file_info['num_samples'] = num_samples

            # æ‰“å°å•æ–‡ä»¶è§£æè¯¦æƒ…
            print(f"   â”œâ”€ å¸§å¤§å°ï¼š{frame_size:,} | æ€»å¸§æ•°ï¼š{total_frames:,} | æ¯å¸§æ ·æœ¬æ•°ï¼š{samples_per_frame:,}")
            print(
                f"   â”œâ”€ æ€»IQå¯¹ï¼š{total_iq:,} | æœ‰æ•ˆIQå¯¹ï¼š{valid_iq:,} | é‡‡æ ·ç‡ï¼š{sample_rate / 1e3:.1f} kSPS" if sample_rate else f"   â”œâ”€ æ€»IQå¯¹ï¼š{total_iq:,} | æœ‰æ•ˆIQå¯¹ï¼š{valid_iq:,}")

        # ========== WAVæ–‡ä»¶è§£æï¼ˆæ ¸å¿ƒï¼šè·³è¿‡1068å­—èŠ‚å¤´ï¼‰ ==========
        elif file_ext == '.wav':
            # è·å–æ–‡ä»¶æ€»å¤§å°
            file_size = os.path.getsize(file_path)
            # è·³è¿‡1068å­—èŠ‚å¤´ï¼Œè¯»å–å‰©ä½™æ•°æ®
            with open(file_path, 'rb') as f:
                f.seek(1068)  # ä¸¥æ ¼æŒ‰è§„èŒƒè·³è¿‡1068å­—èŠ‚ï¼ˆé1086ï¼‰
                data = np.fromfile(f, dtype=np.int16)

            total_iq = len(data) // 2  # è·³è¿‡1068åçš„æ€»IQå¯¹
            file_info['total_iq_pairs'] = total_iq
            file_info['valid_iq_pairs'] = total_iq

            # è®¡ç®—æ ·æœ¬æ•°ï¼šæ€»IQå¯¹ - æ ·æœ¬é•¿åº¦ + 1
            if total_iq < config.SAMPLE_LENGTH:
                print(f"âš ï¸  WAVæ–‡ä»¶{filename}IQå¯¹{total_iq} < æ ·æœ¬é•¿åº¦{config.SAMPLE_LENGTH}ï¼Œæ— æœ‰æ•ˆæ ·æœ¬")
                return None
            num_samples = total_iq - config.SAMPLE_LENGTH + 1
            file_info['num_samples'] = num_samples

            # æ‰“å°å•æ–‡ä»¶è§£æè¯¦æƒ…
            print(f"   â”œâ”€ æ–‡ä»¶æ€»å¤§å°ï¼š{file_size / 1024 / 1024:.2f} MB | è·³è¿‡å¤´éƒ¨ï¼š1068å­—èŠ‚")
            print(
                f"   â”œâ”€ æœ‰æ•ˆIQå¯¹ï¼š{total_iq:,} | é‡‡æ ·ç‡ï¼š{sample_rate / 1e6:.3f} MSPS" if sample_rate else f"   â”œâ”€ æœ‰æ•ˆIQå¯¹ï¼š{total_iq:,}")

        else:
            print(f"âš ï¸  ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š{file_ext}ï¼Œè·³è¿‡")
            return None

        # æœ€ç»ˆæ ¡éªŒæ ·æœ¬æ•°
        if file_info['num_samples'] <= 0:
            print(f"âš ï¸  æ–‡ä»¶{filename}æ— æœ‰æ•ˆæ ·æœ¬ï¼Œè·³è¿‡")
            return None

        print(f"   â””â”€ æœ‰æ•ˆæ ·æœ¬æ•°ï¼š{file_info['num_samples']:,}")
        return file_info

    except Exception as e:
        print(f"âš ï¸  å¤„ç†æ–‡ä»¶å¤±è´¥ï¼š{file_path} -> {str(e)}")
        return None


def stream_read_sample(file_info, start_idx):
    """
    æµå¼è¯»å–å•ä¸ªæ ·æœ¬ï¼ˆä¸ç¼“å­˜ï¼‰
    - BINï¼šç›´æ¥ä»start_idx*4å­—èŠ‚ä½ç½®è¯»å–
    - WAVï¼šä»1068 + start_idx*4å­—èŠ‚ä½ç½®è¯»å–
    """
    file_path = file_info['file_path'].replace('/', '\\')
    if not os.path.exists(file_path):
        file_path = os.path.join(config.DATA_DIR, os.path.basename(file_path))

    # è®¡ç®—è¯»å–èµ·å§‹ä½ç½®
    if file_info['file_type'] == 'bin':
        seek_pos = start_idx * 4  # int16Ã—2=4å­—èŠ‚/IQå¯¹
    elif file_info['file_type'] == 'wav':
        seek_pos = 1068 + start_idx * 4  # è·³è¿‡1068å­—èŠ‚å¤´
    else:
        return np.zeros((config.SAMPLE_LENGTH, 2), dtype=np.float32)

    # è¯»å–IQæ•°æ®
    try:
        with open(file_path, 'rb') as f:
            f.seek(seek_pos)
            data = np.fromfile(f, dtype=np.int16, count=config.SAMPLE_LENGTH * 2)
    except Exception as e:
        print(f"âš ï¸  è¯»å–æ ·æœ¬å¤±è´¥ï¼š{file_info['filename']} start_idx={start_idx} -> {e}")
        return np.zeros((config.SAMPLE_LENGTH, 2), dtype=np.float32)

    # å¤„ç†æ•°æ®ï¼ˆè¡¥é›¶+å½’ä¸€åŒ–ï¼‰
    if len(data) < config.SAMPLE_LENGTH * 2:
        sample = np.zeros((config.SAMPLE_LENGTH, 2), dtype=np.int16)
        valid_len = len(data) // 2
        sample[:valid_len] = data.reshape(-1, 2)
    else:
        sample = data.reshape(-1, 2)

    # å½’ä¸€åŒ–åˆ°[-1, 1]
    sample_norm = sample.astype(np.float32) / 32767.0
    return sample_norm


# ===================== ä¸»å‡½æ•°ï¼šæµå¼æ„é€ æ•°æ®é›† =====================
def construct_dataset_stream():
    """ä¸»å‡½æ•°ï¼šæµå¼æ„é€ æ•°æ®é›†ï¼ˆä¸åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜ï¼‰"""
    total_start_time = time.time()

    # åˆå§‹åŒ–ä¿¡æ¯æ‰“å°
    print("=" * 80)
    print("ğŸš€ å¼€å§‹æµå¼æ„é€ è°ƒåˆ¶ä¿¡å·æ•°æ®é›†")
    print(f"ğŸ“… å¼€å§‹æ—¶é—´ï¼š{time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"âš™ï¸  é…ç½®ï¼šåˆ†å—å¤§å°={config.CHUNK_SIZE:,} | æµ‹è¯•é›†={config.TEST_SIZE:.1%} | éªŒè¯é›†={config.VAL_SIZE:.1%}")
    print(f"ğŸ“ æ•°æ®ç›®å½•ï¼š{config.DATA_DIR} | è¾“å‡ºç›®å½•ï¼š{config.DATASET_OUTPUT_DIR}")
    print("=" * 80)
    print_memory_usage("åˆå§‹åŒ–")

    # ========== ç¬¬ä¸€æ­¥ï¼šç»Ÿè®¡æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯ ==========
    print("\nğŸ“Œ ç¬¬ä¸€æ­¥ï¼šè§£æå¹¶ç»Ÿè®¡æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯...")
    start_time = time.time()
    all_file_metadata = []
    modulation_samples = {}  # æŒ‰è°ƒåˆ¶ç±»å‹å­˜å‚¨æ–‡ä»¶ä¿¡æ¯
    total_samples = 0
    file_count = 0
    valid_file_count = 0

    # éå†æ‰€æœ‰æ–‡ä»¶
    for filename in os.listdir(config.DATA_DIR):
        file_path = os.path.join(config.DATA_DIR, filename)
        if not os.path.isfile(file_path):
            continue

        file_count += 1
        print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶ [{file_count}]ï¼š{filename}")

        # è§£ææ–‡ä»¶ä¿¡æ¯
        file_info = get_file_iq_info(file_path)
        if not file_info:
            continue

        # ç»Ÿè®¡æœ‰æ•ˆæ–‡ä»¶
        valid_file_count += 1
        all_file_metadata.append(file_info)
        modulation = file_info['modulation']

        # æŒ‰è°ƒåˆ¶ç±»å‹åˆ†ç»„
        if modulation not in modulation_samples:
            modulation_samples[modulation] = []
        modulation_samples[modulation].append({
            'file_info': file_info,
            'start_idx': 0,
            'end_idx': file_info['num_samples']
        })

        # ç´¯è®¡æ€»æ ·æœ¬æ•°
        total_samples += file_info['num_samples']
        print(f"   â””â”€ ç´¯è®¡æ€»æ ·æœ¬æ•°ï¼š{total_samples:,}")

    # ç¬¬ä¸€æ­¥æ€»ç»“
    elapsed = time.time() - start_time
    print(f"\nâœ… ç¬¬ä¸€æ­¥å®Œæˆï¼è€—æ—¶ï¼š{elapsed:.2f}s")
    print(
        f"ğŸ“Š ç»Ÿè®¡ç»“æœï¼šæ€»æ–‡ä»¶æ•°={file_count} | æœ‰æ•ˆæ–‡ä»¶æ•°={valid_file_count} | æ€»æ ·æœ¬æ•°={total_samples:,} | è°ƒåˆ¶ç±»å‹æ•°={len(modulation_samples)}")
    print_memory_usage("æ–‡ä»¶è§£æå®Œæˆ")

    # ä¿å­˜æ–‡ä»¶å…ƒæ•°æ®
    meta_path = os.path.join(config.DATASET_OUTPUT_DIR, "file_metadata.csv")
    pd.DataFrame(all_file_metadata).to_csv(meta_path, index=False, encoding='utf-8')
    print(f"ğŸ’¾ ä¿å­˜æ–‡ä»¶å…ƒæ•°æ®ï¼š{meta_path}")

    # ç”Ÿæˆæ ‡ç­¾æ˜ å°„
    all_modulations = sorted(list(modulation_samples.keys()))
    label_encoder_mapping = {mod: idx for idx, mod in enumerate(all_modulations)}
    label_mapping = {
        'label_to_idx': label_encoder_mapping,
        'idx_to_label': {v: k for k, v in label_encoder_mapping.items()},
        'total_samples': total_samples,
        'modulation_count': len(all_modulations),
        'create_time': time.strftime('%Y-%m-%d %H:%M:%S'),
        'config': {
            'sample_length': config.SAMPLE_LENGTH,
            'test_size': config.TEST_SIZE,
            'val_size': config.VAL_SIZE
        }
    }
    label_path = os.path.join(config.DATASET_OUTPUT_DIR, "label_mapping.json")
    with open(label_path, 'w', encoding='utf-8') as f:
        json.dump(label_mapping, f, indent=4, ensure_ascii=False)
    print(f"ğŸ’¾ ä¿å­˜æ ‡ç­¾æ˜ å°„ï¼š{label_path}")

    # æ‰“å°æ ·æœ¬åˆ†å¸ƒ
    print_sample_distribution(modulation_samples, label_encoder_mapping)

    # ========== ç¬¬äºŒæ­¥ï¼šæµå¼åˆ’åˆ†+åˆ†å—å†™å…¥ ==========
    print("\nğŸ“Œ ç¬¬äºŒæ­¥ï¼šæŒ‰è°ƒåˆ¶ç±»å‹åˆ†å±‚åˆ’åˆ†å¹¶åˆ†å—å†™å…¥...")
    start_time = time.time()
    chunk_counter = {'train': 0, 'val': 0, 'test': 0}
    buffer = {
        'train': {'data': [], 'labels': [], 'count': 0},
        'val': {'data': [], 'labels': [], 'count': 0},
        'test': {'data': [], 'labels': [], 'count': 0}
    }
    total_processed = {'train': 0, 'val': 0, 'test': 0}

    # éå†æ¯ä¸ªè°ƒåˆ¶ç±»å‹
    for modulation in tqdm(all_modulations, desc="å¤„ç†è°ƒåˆ¶ç±»å‹", ncols=100):
        mod_start = time.time()
        mod_label = label_encoder_mapping[modulation]
        mod_files = modulation_samples[modulation]

        # æ”¶é›†å½“å‰è°ƒåˆ¶çš„æ‰€æœ‰æ ·æœ¬ç´¢å¼•
        mod_all_samples = []
        for file_item in mod_files:
            file_info = file_item['file_info']
            if file_info['file_type'] == 'bin':
                # BINæ–‡ä»¶ï¼šæŒ‰å¸§ç”Ÿæˆæ ·æœ¬ç´¢å¼•
                frame_size = file_info['frame_size']
                total_frames = file_info['total_frames']
                frame_samples = frame_size - config.SAMPLE_LENGTH + 1
                frame_start = 0
                for frame_idx in range(total_frames):
                    for frame_inner_start in range(frame_samples):
                        global_start = frame_start + frame_inner_start
                        mod_all_samples.append((file_info, global_start))
                    frame_start += frame_size
            else:
                # WAVæ–‡ä»¶ï¼šè¿ç»­æ ·æœ¬ç´¢å¼•
                for start_idx in range(file_info['num_samples']):
                    mod_all_samples.append((file_info, start_idx))

        mod_total = len(mod_all_samples)
        print(f"\nğŸ” å¤„ç†è°ƒåˆ¶ç±»å‹ï¼š{modulation} | æ€»æ ·æœ¬æ•°ï¼š{mod_total:,} | æ ‡ç­¾IDï¼š{mod_label}")

        # åˆ†å±‚åˆ’åˆ†train/val/test
        mod_samples_arr = np.array(mod_all_samples, dtype=object)
        # å…ˆåˆ’åˆ†testé›†
        X_train_val, X_test = train_test_split(
            mod_samples_arr, test_size=config.TEST_SIZE, random_state=config.RANDOM_STATE
        )
        # å†åˆ’åˆ†train/valé›†
        X_train, X_val = train_test_split(
            X_train_val, test_size=config.VAL_SIZE, random_state=config.RANDOM_STATE
        )

        # æ‰“å°åˆ’åˆ†ç»“æœ
        print(f"   â”œâ”€ Trainï¼š{len(X_train):,} ({len(X_train) / mod_total:.1%})")
        print(f"   â”œâ”€ Valï¼š{len(X_val):,} ({len(X_val) / mod_total:.1%})")
        print(f"   â””â”€ Testï¼š{len(X_test):,} ({len(X_test) / mod_total:.1%})")

        # ========== å¤„ç†Trainæ ·æœ¬ ==========
        train_pbar = tqdm(X_train, desc=f"{modulation} - Train", leave=False, ncols=80)
        for sample_item in train_pbar:
            file_info, start_idx = sample_item
            sample_data = stream_read_sample(file_info, start_idx)

            buffer['train']['data'].append(sample_data)
            buffer['train']['labels'].append(mod_label)
            total_processed['train'] += 1

            # ç¼“å†²åŒºæ»¡åˆ™å†™å…¥åˆ†å—
            if len(buffer['train']['data']) >= config.CHUNK_SIZE:
                data_arr = np.array(buffer['train']['data']).transpose(0, 2, 1)
                label_arr = np.array(buffer['train']['labels'])
                chunk_path = os.path.join(temp_chunk_dir, f"train_chunk_{chunk_counter['train']}.npz")
                np.savez(chunk_path, data=data_arr, labels=label_arr)

                # æ‰“å°åˆ†å—ä¿¡æ¯
                chunk_size = os.path.getsize(chunk_path) / 1024 / 1024
                print(
                    f"\nğŸ“¦ å†™å…¥Trainåˆ†å— [{chunk_counter['train']}]ï¼š{chunk_path} | æ ·æœ¬æ•°ï¼š{len(data_arr):,} | å¤§å°ï¼š{chunk_size:.2f} MB")

                # é‡ç½®ç¼“å†²åŒº
                buffer['train']['data'] = []
                buffer['train']['labels'] = []
                chunk_counter['train'] += 1
                print_memory_usage(f"Trainåˆ†å—{chunk_counter['train']}")

        # ========== å¤„ç†Valæ ·æœ¬ ==========
        val_pbar = tqdm(X_val, desc=f"{modulation} - Val", leave=False, ncols=80)
        for sample_item in val_pbar:
            file_info, start_idx = sample_item
            sample_data = stream_read_sample(file_info, start_idx)

            buffer['val']['data'].append(sample_data)
            buffer['val']['labels'].append(mod_label)
            total_processed['val'] += 1

            if len(buffer['val']['data']) >= config.CHUNK_SIZE:
                data_arr = np.array(buffer['val']['data']).transpose(0, 2, 1)
                label_arr = np.array(buffer['val']['labels'])
                chunk_path = os.path.join(temp_chunk_dir, f"val_chunk_{chunk_counter['val']}.npz")
                np.savez(chunk_path, data=data_arr, labels=label_arr)

                chunk_size = os.path.getsize(chunk_path) / 1024 / 1024
                print(
                    f"\nğŸ“¦ å†™å…¥Valåˆ†å— [{chunk_counter['val']}]ï¼š{chunk_path} | æ ·æœ¬æ•°ï¼š{len(data_arr):,} | å¤§å°ï¼š{chunk_size:.2f} MB")

                buffer['val']['data'] = []
                buffer['val']['labels'] = []
                chunk_counter['val'] += 1
                print_memory_usage(f"Valåˆ†å—{chunk_counter['val']}")

        # ========== å¤„ç†Testæ ·æœ¬ ==========
        test_pbar = tqdm(X_test, desc=f"{modulation} - Test", leave=False, ncols=80)
        for sample_item in test_pbar:
            file_info, start_idx = sample_item
            sample_data = stream_read_sample(file_info, start_idx)

            buffer['test']['data'].append(sample_data)
            buffer['test']['labels'].append(mod_label)
            total_processed['test'] += 1

            if len(buffer['test']['data']) >= config.CHUNK_SIZE:
                data_arr = np.array(buffer['test']['data']).transpose(0, 2, 1)
                label_arr = np.array(buffer['test']['labels'])
                chunk_path = os.path.join(temp_chunk_dir, f"test_chunk_{chunk_counter['test']}.npz")
                np.savez(chunk_path, data=data_arr, labels=label_arr)

                chunk_size = os.path.getsize(chunk_path) / 1024 / 1024
                print(
                    f"\nğŸ“¦ å†™å…¥Teståˆ†å— [{chunk_counter['test']}]ï¼š{chunk_path} | æ ·æœ¬æ•°ï¼š{len(data_arr):,} | å¤§å°ï¼š{chunk_size:.2f} MB")

                buffer['test']['data'] = []
                buffer['test']['labels'] = []
                chunk_counter['test'] += 1
                print_memory_usage(f"Teståˆ†å—{chunk_counter['test']}")

        # è°ƒåˆ¶ç±»å‹å¤„ç†å®Œæˆ
        mod_elapsed = time.time() - mod_start
        print(f"âœ… å®Œæˆè°ƒåˆ¶ç±»å‹ {modulation} | è€—æ—¶ï¼š{mod_elapsed:.2f}s | é€Ÿåº¦ï¼š{mod_total / mod_elapsed:.0f} æ ·æœ¬/ç§’")

    # ç¬¬äºŒæ­¥æ€»ç»“
    elapsed = time.time() - start_time
    print(f"\nâœ… ç¬¬äºŒæ­¥å®Œæˆï¼è€—æ—¶ï¼š{elapsed:.2f}s")
    print(f"ğŸ“Š åˆ†å—ç»Ÿè®¡ï¼šTrain={chunk_counter['train']} | Val={chunk_counter['val']} | Test={chunk_counter['test']}")
    print(
        f"ğŸ“Š å¤„ç†æ ·æœ¬ï¼šTrain={total_processed['train']:,} | Val={total_processed['val']:,} | Test={total_processed['test']:,}")
    print_memory_usage("åˆ†å—å†™å…¥å®Œæˆ")

    # ========== ç¬¬ä¸‰æ­¥ï¼šå†™å…¥å‰©ä½™æ ·æœ¬ ==========
    print("\nğŸ“Œ ç¬¬ä¸‰æ­¥ï¼šå†™å…¥å‰©ä½™ç¼“å†²åŒºæ ·æœ¬...")
    start_time = time.time()
    remaining_total = 0

    for split in ['train', 'val', 'test']:
        if len(buffer[split]['data']) > 0:
            data_arr = np.array(buffer[split]['data']).transpose(0, 2, 1)
            label_arr = np.array(buffer[split]['labels'])
            remaining_total += len(data_arr)

            chunk_path = os.path.join(temp_chunk_dir, f"{split}_chunk_{chunk_counter[split]}.npz")
            np.savez(chunk_path, data=data_arr, labels=label_arr)

            chunk_size = os.path.getsize(chunk_path) / 1024 / 1024
            print(
                f"ğŸ“¦ å†™å…¥{split}å‰©ä½™åˆ†å— [{chunk_counter[split]}]ï¼š{chunk_path} | æ ·æœ¬æ•°ï¼š{len(data_arr):,} | å¤§å°ï¼š{chunk_size:.2f} MB")

            chunk_counter[split] += 1

    elapsed = time.time() - start_time
    print(f"\nâœ… ç¬¬ä¸‰æ­¥å®Œæˆï¼è€—æ—¶ï¼š{elapsed:.2f}s | å†™å…¥å‰©ä½™æ ·æœ¬ï¼š{remaining_total:,}")
    print_memory_usage("å‰©ä½™æ ·æœ¬å†™å…¥å®Œæˆ")

    # ========== ç¬¬å››æ­¥ï¼šåˆå¹¶åˆ†å—æ–‡ä»¶ ==========
    print("\nğŸ“Œ ç¬¬å››æ­¥ï¼šåˆå¹¶åˆ†å—æ–‡ä»¶...")
    start_time = time.time()
    final_sample_count = {}

    for split in ['train', 'val', 'test']:
        split_start = time.time()
        print(f"\nğŸ”— åˆå¹¶{split}é›†ï¼ˆå…±{chunk_counter[split]}ä¸ªåˆ†å—ï¼‰...")

        all_data = []
        all_labels = []

        # éå†æ‰€æœ‰åˆ†å—
        for chunk_idx in tqdm(range(chunk_counter[split]), desc=f"{split}åˆå¹¶è¿›åº¦", ncols=80):
            chunk_path = os.path.join(temp_chunk_dir, f"{split}_chunk_{chunk_idx}.npz")
            if not os.path.exists(chunk_path):
                print(f"âš ï¸  åˆ†å—ä¸å­˜åœ¨ï¼š{chunk_path}ï¼Œè·³è¿‡")
                continue

            # è¯»å–åˆ†å—
            chunk_data = np.load(chunk_path, allow_pickle=True)
            all_data.append(chunk_data['data'])
            all_labels.append(chunk_data['labels'])

            # åˆ é™¤ä¸´æ—¶åˆ†å—
            os.remove(chunk_path)

        # åˆå¹¶å¹¶ä¿å­˜
        if len(all_data) > 0:
            final_data = np.concatenate(all_data, axis=0)
            final_labels = np.concatenate(all_labels, axis=0)
            final_sample_count[split] = len(final_data)

            # ä¿å­˜æœ€ç»ˆæ–‡ä»¶
            data_path = os.path.join(config.DATASET_OUTPUT_DIR, f"{split}_data.npy")
            label_path = os.path.join(config.DATASET_OUTPUT_DIR, f"{split}_labels.npy")
            np.save(data_path, final_data)
            np.save(label_path, final_labels)

            # æ‰“å°æ–‡ä»¶ä¿¡æ¯
            data_size = os.path.getsize(data_path) / 1024 / 1024 / 1024
            label_size = os.path.getsize(label_path) / 1024 / 1024
            split_elapsed = time.time() - split_start

            print(f"âœ… {split}é›†åˆå¹¶å®Œæˆï¼")
            print(f"   â”œâ”€ æ ·æœ¬æ•°ï¼š{len(final_data):,}")
            print(f"   â”œâ”€ æ•°æ®æ–‡ä»¶ï¼š{data_path} | å¤§å°ï¼š{data_size:.2f} GB")
            print(f"   â”œâ”€ æ ‡ç­¾æ–‡ä»¶ï¼š{label_path} | å¤§å°ï¼š{label_size:.2f} MB")
            print(f"   â””â”€ è€—æ—¶ï¼š{split_elapsed:.2f}s")
        else:
            final_sample_count[split] = 0
            print(f"âš ï¸ {split}é›†æ— åˆ†å—å¯åˆå¹¶ï¼")

    # åˆ é™¤ä¸´æ—¶ç›®å½•
    if os.path.exists(temp_chunk_dir) and len(os.listdir(temp_chunk_dir)) == 0:
        os.rmdir(temp_chunk_dir)
        print(f"\nğŸ—‘ï¸ åˆ é™¤ä¸´æ—¶ç›®å½•ï¼š{temp_chunk_dir}")
    else:
        print(f"\nâš ï¸ ä¸´æ—¶ç›®å½•æœªç©ºï¼š{temp_chunk_dir} | å‰©ä½™æ–‡ä»¶ï¼š{len(os.listdir(temp_chunk_dir))}")

    # ç¬¬å››æ­¥æ€»ç»“
    elapsed = time.time() - start_time
    print(f"\nâœ… ç¬¬å››æ­¥å®Œæˆï¼è€—æ—¶ï¼š{elapsed:.2f}s")
    print_memory_usage("åˆ†å—åˆå¹¶å®Œæˆ")

    # ========== æœ€ç»ˆæ±‡æ€» ==========
    total_elapsed = time.time() - total_start_time
    print("\n" + "=" * 80)
    print("ğŸ‰ è°ƒåˆ¶ä¿¡å·æ•°æ®é›†æ„é€ å®Œæˆï¼")
    print(f"ğŸ“… ç»“æŸæ—¶é—´ï¼š{time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"â±ï¸  æ€»è€—æ—¶ï¼š{total_elapsed:.2f}s ({total_elapsed / 60:.1f}åˆ†é’Ÿ)")
    print(f"ğŸš€ å¹³å‡é€Ÿåº¦ï¼š{total_samples / total_elapsed:.0f} æ ·æœ¬/ç§’")

    # æœ€ç»ˆç»Ÿè®¡æŠ¥è¡¨
    print("\nğŸ“Š æœ€ç»ˆæ•°æ®é›†ç»Ÿè®¡ï¼š")
    print(f"{'æ•°æ®é›†':<10} {'æ ·æœ¬æ•°':<15} {'æ•°æ®æ–‡ä»¶å¤§å°':<15} {'æ ‡ç­¾æ–‡ä»¶å¤§å°':<15}")
    print("-" * 65)
    total_final = 0
    for split in ['train', 'val', 'test']:
        data_path = os.path.join(config.DATASET_OUTPUT_DIR, f"{split}_data.npy")
        label_path = os.path.join(config.DATASET_OUTPUT_DIR, f"{split}_labels.npy")

        if os.path.exists(data_path):
            data_size = os.path.getsize(data_path) / 1024 / 1024 / 1024
            label_size = os.path.getsize(label_path) / 1024 / 1024 if os.path.exists(label_path) else 0
            count = final_sample_count.get(split, 0)
            total_final += count
            print(f"{split:<10} {count:<15,} {data_size:<15.2f} GB {label_size:<15.2f} MB")
        else:
            print(f"{split:<10} 0:<15, {'-':<15} {'-':<15}")
    print("-" * 65)
    print(f"{'æ€»è®¡':<10} {total_final:<15,} {'-':<15} {'-':<15}")

    # ç”Ÿæˆæ–‡ä»¶åˆ—è¡¨
    print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨ï¼š")
    output_files = [
        "file_metadata.csv", "label_mapping.json",
        "train_data.npy", "train_labels.npy",
        "val_data.npy", "val_labels.npy",
        "test_data.npy", "test_labels.npy"
    ]
    for f in output_files:
        fp = os.path.join(config.DATASET_OUTPUT_DIR, f)
        if os.path.exists(fp):
            size = os.path.getsize(fp)
            size_unit = "GB" if size > 1024 * 1024 * 1024 else "MB"
            size_val = size / 1024 / 1024 / 1024 if size_unit == "GB" else size / 1024 / 1024
            print(f"  âœ… {f:<20} | å¤§å°ï¼š{size_val:.2f} {size_unit}")
        else:
            print(f"  âŒ {f:<20} | æ–‡ä»¶ä¸å­˜åœ¨")

    print("\nâœ… å…¨ç¨‹æœªåŠ è½½æ‰€æœ‰æ ·æœ¬åˆ°å†…å­˜ï¼Œ48Gå†…å­˜å®Œå…¨é€‚é…ï¼")
    print("âœ… å¯è¿è¡Œtrainer.pyå¼€å§‹æ¨¡å‹è®­ç»ƒï¼")
    print("=" * 80)
    print_memory_usage("ä»»åŠ¡å®Œæˆ")


# ===================== è¿è¡Œå…¥å£ =====================
if __name__ == "__main__":
    # æ‰“å°ç³»ç»Ÿä¿¡æ¯
    print("ğŸ–¥ï¸  ç³»ç»Ÿç¯å¢ƒä¿¡æ¯ï¼š")
    print(f"   â”œâ”€ NumPyç‰ˆæœ¬ï¼š{np.__version__}")  # ä¿®å¤åŸä»£ç Pythonç‰ˆæœ¬æ‰“å°é”™è¯¯
    print(f"   â”œâ”€ CPUæ ¸å¿ƒæ•°ï¼š{psutil.cpu_count(logical=True)}")
    print(f"   â”œâ”€ æ€»å†…å­˜ï¼š{psutil.virtual_memory().total / 1024 / 1024 / 1024:.2f} GB")
    print(f"   â”œâ”€ å¯ç”¨å†…å­˜ï¼š{psutil.virtual_memory().available / 1024 / 1024 / 1024:.2f} GB")
    print(f"   â””â”€ ç£ç›˜å¯ç”¨ç©ºé—´ï¼š{psutil.disk_usage(config.DATASET_OUTPUT_DIR).free / 1024 / 1024 / 1024:.2f} GB")

    # å¯åŠ¨æ•°æ®é›†æ„é€ 
    construct_dataset_stream()