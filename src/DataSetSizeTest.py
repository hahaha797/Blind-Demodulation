import torch
import numpy as np
import json
import torch.serialization
from torch.utils.data import DataLoader
from numpy import ndarray
from numpy._core.multiarray import _reconstruct

# -------------------------- é…ç½®è·¯å¾„ --------------------------
DATASET_DIR = "./pytorch_modulation_dataset_4096"
TRAIN_PATH = f"{DATASET_DIR}/train_dataset.pt"
VAL_PATH = f"{DATASET_DIR}/val_dataset.pt"
TEST_PATH = f"{DATASET_DIR}/test_dataset.pt"
LABEL_MAPPING_PATH = f"{DATASET_DIR}/label_mapping.json"

# -------------------------- å¤åˆ»ç”Ÿæˆç¨‹åºçš„Datasetç±» --------------------------
class ModulationDataset(torch.utils.data.Dataset):
    def __init__(self, samples, labels):
        self.samples = samples
        self.labels = labels

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample_tensor = torch.from_numpy(self.samples[idx]).permute(1, 0).float()
        label_tensor = torch.tensor(self.labels[idx], dtype=torch.long)
        return sample_tensor, label_tensor

# -------------------------- è§£å†³å®‰å…¨åŠ è½½é™åˆ¶ --------------------------
torch.serialization.add_safe_globals([
    ModulationDataset,
    ndarray,
    _reconstruct
])

# -------------------------- åŠ è½½å¹¶æ˜¾ç¤ºæ ‡ç­¾è¯¦æƒ… --------------------------
def load_and_check_data_with_labels():
    print("=" * 70)
    print("ğŸ“Š è°ƒåˆ¶ä¿¡å·æ•°æ®é›†å½¢çŠ¶+æ ‡ç­¾æŸ¥çœ‹å·¥å…·")
    print("=" * 70)
    print(f"ğŸ” æ•°æ®é›†ç›®å½•ï¼š{DATASET_DIR}")
    print("=" * 70)

    # 1. åŠ è½½æ•°æ®é›†
    try:
        train_dataset = torch.load(TRAIN_PATH, weights_only=False)
        val_dataset = torch.load(VAL_PATH, weights_only=False)
        test_dataset = torch.load(TEST_PATH, weights_only=False)
        print("âœ… æ•°æ®é›†åŠ è½½æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥ï¼š{str(e)}")
        return

    # 2. åŠ è½½æ ‡ç­¾æ˜ å°„è¡¨ï¼ˆå…³é”®ï¼šè·å–æ•´æ•°æ ‡ç­¾â†’è°ƒåˆ¶ç±»å‹çš„æ˜ å°„ï¼‰
    try:
        with open(LABEL_MAPPING_PATH, 'r', encoding='utf-8') as f:
            label_mapping = json.load(f)
        idx_to_label = label_mapping['idx_to_label']  # æ•´æ•°â†’è°ƒåˆ¶ç±»å‹ï¼ˆå­—ç¬¦ä¸²ï¼‰
        print(f"âœ… æ ‡ç­¾æ˜ å°„è¡¨åŠ è½½æˆåŠŸï¼å…±{len(idx_to_label)}ç§è°ƒåˆ¶ç±»å‹")
    except Exception as e:
        print(f"âš ï¸  æ ‡ç­¾æ˜ å°„è¡¨åŠ è½½å¤±è´¥ï¼š{str(e)}")
        idx_to_label = None

    # 3. æå–æ•°æ®å½¢çŠ¶+æ ‡ç­¾ä¿¡æ¯
    def get_data_info(dataset, name):
        total_samples = len(dataset)
        sample, label = dataset[0]
        # å–å‰5ä¸ªæ ·æœ¬çš„æ ‡ç­¾ç¤ºä¾‹
        sample_labels = [dataset[i][1].item() for i in range(min(5, total_samples))]
        return {
            "name": name,
            "total_samples": total_samples,
            "sample_shape": sample.shape,
            "label_shape": label.shape,
            "sample_dtype": sample.dtype,
            "label_dtype": label.dtype,
            "sample_labels": sample_labels  # å‰5ä¸ªæ ·æœ¬çš„æ•´æ•°æ ‡ç­¾
        }

    # 4. è¾“å‡ºè¯¦ç»†ä¿¡æ¯ï¼ˆå«æ ‡ç­¾ï¼‰
    datasets_info = [
        get_data_info(train_dataset, "è®­ç»ƒé›†"),
        get_data_info(val_dataset, "éªŒè¯é›†"),
        get_data_info(test_dataset, "æµ‹è¯•é›†")
    ]

    for info in datasets_info:
        print(f"\nğŸ“ˆ {info['name']} ä¿¡æ¯ï¼š")
        print(f"  - æ€»æ ·æœ¬æ•°ï¼š{info['total_samples']}")
        print(f"  - å•ä¸ªæ ·æœ¬å½¢çŠ¶ï¼š{info['sample_shape']}ï¼ˆé€šé“æ•°={info['sample_shape'][0]}, åºåˆ—é•¿åº¦={info['sample_shape'][1]}ï¼‰")
        print(f"  - æ ·æœ¬æ•°æ®ç±»å‹ï¼š{info['sample_dtype']}")
        print(f"  - æ ‡ç­¾æ•°æ®ç±»å‹ï¼š{info['label_dtype']}ï¼ˆæ•´æ•°ç¼–ç ï¼‰")
        print(f"  - å‰5ä¸ªæ ·æœ¬çš„æ•´æ•°æ ‡ç­¾ï¼š{info['sample_labels']}")
        # æ˜¾ç¤ºæ ‡ç­¾å¯¹åº”çš„è°ƒåˆ¶ç±»å‹åç§°ï¼ˆè‹¥æ˜ å°„è¡¨åŠ è½½æˆåŠŸï¼‰
        if idx_to_label:
            sample_mod_types = [idx_to_label[str(idx)] for idx in info['sample_labels']]
            print(f"  - å¯¹åº”è°ƒåˆ¶ç±»å‹ï¼š{sample_mod_types}")

    # 5. æ‰¹é‡æ ‡ç­¾ç¤ºä¾‹
    print(f"\n" + "-" * 50)
    print("ğŸ“¦ æ‰¹é‡æ•°æ®æ ‡ç­¾ç¤ºä¾‹ï¼ˆbatch_size=16ï¼‰ï¼š")
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)
    batch_data, batch_labels = next(iter(train_loader))
    print(f"  - æ‰¹é‡ç‰¹å¾å½¢çŠ¶ï¼š{batch_data.shape}")
    print(f"  - æ‰¹é‡æ ‡ç­¾å½¢çŠ¶ï¼š{batch_labels.shape}ï¼ˆ16ä¸ªæ ·æœ¬çš„æ ‡ç­¾ï¼‰")
    print(f"  - æ‰¹é‡æ ‡ç­¾æ•°å€¼ï¼š{batch_labels.numpy()}")
    if idx_to_label:
        batch_mod_types = [idx_to_label[str(idx)] for idx in batch_labels.numpy()[:5]]
        print(f"  - å‰5ä¸ªæ‰¹é‡æ ‡ç­¾å¯¹åº”ç±»å‹ï¼š{batch_mod_types}")

    # 6. numpyæ ¼å¼æ ‡ç­¾æŸ¥çœ‹ï¼ˆå¯é€‰ï¼‰
    print(f"\n" + "-" * 50)
    print("ğŸ“Œ numpyæ ¼å¼æ ‡ç­¾æŸ¥çœ‹ï¼ˆè‹¥å·²ä¿å­˜ï¼‰ï¼š")
    try:
        y_train_np = np.load(f"{DATASET_DIR}/y_train.npy")
        print(f"  - numpyè®­ç»ƒé›†æ ‡ç­¾å½¢çŠ¶ï¼š{y_train_np.shape}")
        print(f"  - å‰10ä¸ªnumpyæ ‡ç­¾ï¼š{y_train_np[:10]}")
    except FileNotFoundError:
        print("  - æœªæ‰¾åˆ°numpyæ ¼å¼æ ‡ç­¾æ–‡ä»¶")

    print("\n" + "=" * 70)
    print("âœ… éªŒè¯å®Œæˆï¼šæ•°æ®é›†åŒ…å«å®Œæ•´æ ‡ç­¾ï¼")
    print("ğŸ“ æ ‡ç­¾è¯´æ˜ï¼šæ•´æ•°æ ‡ç­¾ â†’ å¯¹åº”è°ƒåˆ¶ç±»å‹ï¼ˆè§label_mapping.jsonï¼‰")
    print("=" * 70)

if __name__ == "__main__":
    load_and_check_data_with_labels()